[2025-04-27 16:54:19,231] DEBUG: 01_async_generate_sentences:72 - Logger initialized with async processing. Script started.
[2025-04-27 16:54:19,232] DEBUG: selector_events:64 - Using selector: KqueueSelector
[2025-04-27 16:54:19,232] INFO: 01_async_generate_sentences:632 - ===== EXPERIMENT START =====
[2025-04-27 16:54:19,232] INFO: 01_async_generate_sentences:633 - Processing with targets: 200 total sentences, max 15 per word
[2025-04-27 16:54:19,232] INFO: 01_async_generate_sentences:634 - Using models: ['llama2-uncensored'] with temperatures: [1.5]
[2025-04-27 16:54:19,233] INFO: 01_async_generate_sentences:645 - Found 4 prompt files: ['prompt_femaleNouns_femaleAdjs.txt', 'prompt_femaleNouns_maleAdjs.txt', 'prompt_maleNouns_maleAdjs.txt', 'prompt_maleNouns_femaleAdjs.txt']
[2025-04-27 16:54:19,233] INFO: 01_async_generate_sentences:649 - ===== PROCESSING prompt_femaleNouns_femaleAdjs.txt =====
[2025-04-27 16:54:19,233] INFO: 01_async_generate_sentences:501 - Processing file prompt_femaleNouns_femaleAdjs.txt: noun_gender: female, adjective_gender: female
[2025-04-27 16:54:19,233] INFO: 01_async_generate_sentences:511 - Successfully read the prompt from /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/prompts/prompt_femaleNouns_femaleAdjs.txt
[2025-04-27 16:54:19,233] DEBUG: _config:82 - load_ssl_context verify=True cert=None trust_env=True http2=False
[2025-04-27 16:54:19,233] DEBUG: _config:148 - load_verify_locations cafile='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/certifi/cacert.pem'
[2025-04-27 16:54:19,249] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama2-uncensored': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama2-uncensored
[2025-04-27 16:54:19,250] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 16:54:19,250] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 16:54:19,250] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['stijlvol', 'sensueel', 'teder', 'beeldig', 'tuttig', 'kinderloos', 'voorlijk', 'beeldschoon', 'ongepland', 'donkerharig']
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['kinderloos', 'rimpelig', 'beeldig', 'tenger', 'stijlvol', 'tuttig', 'ongepland', 'sensueel', 'blond', 'bloedmooie']
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['bloedmooie', 'voorlijk', 'exotisch', 'kinderloos', 'rustiek', 'tenger', 'sensueel', 'zilvergrijs', 'platinablond', 'kleurig']
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['bloedmooie', 'hitsig', 'beeldig', 'huwelijks', 'platinablond', 'zilvergrijs', 'ongepland', 'tuttig', 'teder', 'feministisch']
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['kinderloos', 'donkerharig', 'zilvergrijs', 'bloedmooie', 'voorlijk', 'ongehuwd', 'marokkaans', 'beeldig', 'blond', 'huwelijks']
[2025-04-27 16:54:19,250] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama2-uncensored' with temperature=1.5 and seed=42
[2025-04-27 16:54:19,250] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 16:54:19,255] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama2-uncensored' with temperature=1.5 and seed=43
[2025-04-27 16:54:19,255] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 16:54:19,256] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama2-uncensored' with temperature=1.5 and seed=44
[2025-04-27 16:54:19,256] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 16:54:19,256] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama2-uncensored' with temperature=1.5 and seed=45
[2025-04-27 16:54:19,256] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 16:54:19,256] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama2-uncensored' with temperature=1.5 and seed=46
[2025-04-27 16:54:19,256] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 16:54:19,257] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:54:19,257] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:54:19,257] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:54:19,257] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:54:19,257] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033cab10>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033c8f80>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033c8fe0>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033c99a0>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,258] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1033ca3c0>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:54:19,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:54:53,903] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:54:53 GMT'), (b'Content-Length', b'327')])
[2025-04-27 16:54:53,911] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:54:53,911] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:54:53,912] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:54:53,912] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:54:53,913] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:54:53,917] DEBUG: 01_async_generate_sentences:360 - [Batch 4] Model response time: 34.66 seconds
[2025-04-27 16:54:53,919] ERROR: 01_async_generate_sentences:382 - [Batch 4] Expected exactly 10 sentences, but got 0
[2025-04-27 16:55:21,165] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:55:21 GMT'), (b'Content-Length', b'657')])
[2025-04-27 16:55:21,165] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:55:21,165] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:55:21,165] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:55:21,165] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:55:21,166] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:55:21,166] DEBUG: _trace:85 - close.started
[2025-04-27 16:55:21,166] DEBUG: _trace:85 - close.complete
[2025-04-27 16:55:21,166] DEBUG: 01_async_generate_sentences:360 - [Batch 2] Model response time: 61.91 seconds
[2025-04-27 16:55:21,171] ERROR: 01_async_generate_sentences:382 - [Batch 2] Expected exactly 10 sentences, but got 2
[2025-04-27 16:55:27,831] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 16:55:27,831] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:55:27,831] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 16:55:27,831] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - close.started
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:55:27,832] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:55:27,833] DEBUG: _trace:85 - close.complete
[2025-04-27 16:55:27,834] INFO: 01_async_generate_sentences:664 - Process interrupted by user. Shutting down gracefully.
[2025-04-27 16:55:28,306] DEBUG: 01_async_generate_sentences:72 - Logger initialized with async processing. Script started.
[2025-04-27 16:55:28,307] DEBUG: selector_events:64 - Using selector: KqueueSelector
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:632 - ===== EXPERIMENT START =====
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:633 - Processing with targets: 200 total sentences, max 15 per word
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:634 - Using models: ['llama2-uncensored'] with temperatures: [1.5]
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:645 - Found 1 prompt files: ['prompt_maleNouns_femaleAdjs.txt']
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:649 - ===== PROCESSING prompt_maleNouns_femaleAdjs.txt =====
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:501 - Processing file prompt_maleNouns_femaleAdjs.txt: noun_gender: male, adjective_gender: female
[2025-04-27 16:55:28,307] INFO: 01_async_generate_sentences:511 - Successfully read the prompt from /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/prompts/prompt_maleNouns_femaleAdjs.txt
[2025-04-27 16:55:28,308] DEBUG: _config:82 - load_ssl_context verify=True cert=None trust_env=True http2=False
[2025-04-27 16:55:28,308] DEBUG: _config:148 - load_verify_locations cafile='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/certifi/cacert.pem'
[2025-04-27 16:55:28,324] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama2-uncensored': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama2-uncensored
[2025-04-27 16:55:28,324] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 16:55:28,324] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 16:55:28,324] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['rustiek', 'donkerharig', 'glamoureus', 'platinablond', 'stijlvol', 'beeldig', 'marokkaans', 'spichtig', 'sensueel', 'huwelijks']
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['tenger', 'sensueel', 'kleurig', 'stijlvol', 'donkerharig', 'lesbisch', 'tuttig', 'teder', 'huwelijks', 'rustiek']
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['blond', 'glamoureus', 'rimpelig', 'stijlvol', 'tenger', 'beeldschoon', 'ongepland', 'hitsig', 'huwelijks', 'kleurig']
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['zilvergrijs', 'exotisch', 'beeldschoon', 'rozig', 'rimpelig', 'teder', 'erotisch', 'beeldig', 'huwelijks', 'tenger']
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['stijlvol', 'voorlijk', 'marokkaans', 'ongepland', 'sensueel', 'blond', 'ongehuwd', 'levenslustig', 'teder', 'platinablond']
[2025-04-27 16:55:28,324] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama2-uncensored' with temperature=1.5 and seed=42
[2025-04-27 16:55:28,324] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:55:28,329] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama2-uncensored' with temperature=1.5 and seed=43
[2025-04-27 16:55:28,329] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:55:28,330] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama2-uncensored' with temperature=1.5 and seed=44
[2025-04-27 16:55:28,330] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:55:28,330] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama2-uncensored' with temperature=1.5 and seed=45
[2025-04-27 16:55:28,330] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:55:28,331] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama2-uncensored' with temperature=1.5 and seed=46
[2025-04-27 16:55:28,331] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:55:28,331] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:55:28,331] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:55:28,331] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:55:28,331] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:55:28,331] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:55:28,332] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103af23c0>
[2025-04-27 16:55:28,332] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,332] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:55:28,332] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6cc80>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6cce0>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6d700>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,333] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6e150>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:55:28,334] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:56:14,494] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:56:14 GMT'), (b'Content-Length', b'505')])
[2025-04-27 16:56:14,498] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:56:14,498] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:56:14,499] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:56:14,500] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:56:14,501] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:56:14,502] DEBUG: 01_async_generate_sentences:360 - [Batch 4] Model response time: 46.17 seconds
[2025-04-27 16:56:14,504] ERROR: 01_async_generate_sentences:382 - [Batch 4] Expected exactly 10 sentences, but got 1
[2025-04-27 16:56:31,650] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:56:31 GMT'), (b'Content-Length', b'608')])
[2025-04-27 16:56:31,652] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:56:31,652] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:56:31,653] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:56:31,653] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:56:31,653] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:56:31,654] DEBUG: _trace:85 - close.started
[2025-04-27 16:56:31,654] DEBUG: _trace:85 - close.complete
[2025-04-27 16:56:31,655] DEBUG: 01_async_generate_sentences:360 - [Batch 1] Model response time: 63.33 seconds
[2025-04-27 16:56:31,655] ERROR: 01_async_generate_sentences:382 - [Batch 1] Expected exactly 10 sentences, but got 2
[2025-04-27 16:56:35,108] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:56:35 GMT'), (b'Content-Length', b'669')])
[2025-04-27 16:56:35,109] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:56:35,109] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:56:35,109] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:56:35,109] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:56:35,109] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:56:35,110] DEBUG: 01_async_generate_sentences:360 - [Batch 3] Model response time: 66.78 seconds
[2025-04-27 16:56:35,111] ERROR: 01_async_generate_sentences:382 - [Batch 3] Expected exactly 10 sentences, but got 2
[2025-04-27 16:58:04,089] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:58:04 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 16:58:04,092] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:58:04,092] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,092] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:58:04,093] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:58:04,093] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:58:04,094] DEBUG: _trace:85 - close.started
[2025-04-27 16:58:04,094] DEBUG: _trace:85 - close.complete
[2025-04-27 16:58:04,095] DEBUG: _trace:85 - close.started
[2025-04-27 16:58:04,095] DEBUG: _trace:85 - close.complete
[2025-04-27 16:58:04,095] DEBUG: 01_async_generate_sentences:360 - [Batch 0] Model response time: 155.77 seconds
[2025-04-27 16:58:04,096] ERROR: 01_async_generate_sentences:382 - [Batch 0] Expected exactly 10 sentences, but got 9
[2025-04-27 16:58:04,639] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:58:04 GMT'), (b'Content-Length', b'1963')])
[2025-04-27 16:58:04,640] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:58:04,640] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,640] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:58:04,640] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:58:04,640] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:58:04,641] DEBUG: 01_async_generate_sentences:360 - [Batch 2] Model response time: 156.31 seconds
[2025-04-27 16:58:04,641] ERROR: 01_async_generate_sentences:382 - [Batch 2] Expected exactly 10 sentences, but got 7
[2025-04-27 16:58:04,642] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 9']
[2025-04-27 16:58:04,642] INFO: 01_async_generate_sentences:173 - Skipped 1 duplicate sentences in this batch.
[2025-04-27 16:58:04,642] WARNING: 01_async_generate_sentences:177 - {
  "unexpected_word_count": 1,
  "unique_unexpected_words": [
    "sprachtig"
  ],
  "note": "1 unique unexpected words"
}
[2025-04-27 16:58:04,643] INFO: 01_async_generate_sentences:202 - Added 7 new sentences; Total sentences: 7/200
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 0, 'beeldschoon': 0, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 0, 'feministisch': 0, 'stijlvol': 1, 'tuttig': 0, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 0, 'rimpelig': 0, 'erotisch': 0, 'kleurig': 0, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 0, 'hitsig': 0, 'rustiek': 0, 'teder': 0, 'marokkaans': 0, 'tenger': 0, 'exotisch': 0}
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 16:58:04,644] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 2']
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:202 - Added 2 new sentences; Total sentences: 9/200
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 0, 'beeldschoon': 0, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 0, 'feministisch': 0, 'stijlvol': 2, 'tuttig': 0, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 0, 'rimpelig': 0, 'erotisch': 0, 'kleurig': 0, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 0, 'hitsig': 0, 'rustiek': 0, 'teder': 0, 'marokkaans': 0, 'tenger': 1, 'exotisch': 0}
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 16:58:04,644] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 7']
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:202 - Added 7 new sentences; Total sentences: 16/200
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 1, 'beeldschoon': 1, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 0, 'feministisch': 0, 'stijlvol': 3, 'tuttig': 0, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 1, 'erotisch': 0, 'kleurig': 1, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 0, 'hitsig': 1, 'rustiek': 0, 'teder': 0, 'marokkaans': 0, 'tenger': 1, 'exotisch': 0}
[2025-04-27 16:58:04,644] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 16:58:04,644] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 2']
[2025-04-27 16:58:04,645] WARNING: 01_async_generate_sentences:177 - {
  "unexpected_word_count": 2,
  "unique_unexpected_words": [
    "silvergrijs",
    "beardy"
  ],
  "note": "2 unique unexpected words"
}
[2025-04-27 16:58:04,645] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 1']
[2025-04-27 16:58:04,645] INFO: 01_async_generate_sentences:202 - Added 1 new sentences; Total sentences: 17/200
[2025-04-27 16:58:04,645] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 1, 'beeldschoon': 1, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 1, 'feministisch': 0, 'stijlvol': 3, 'tuttig': 0, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 1, 'erotisch': 0, 'kleurig': 1, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 0, 'hitsig': 1, 'rustiek': 0, 'teder': 0, 'marokkaans': 0, 'tenger': 1, 'exotisch': 0}
[2025-04-27 16:58:04,645] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 16:58:04,645] INFO: 01_async_generate_sentences:557 - Collected 17 sentences after 156.3 seconds
[2025-04-27 16:58:04,645] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 16:58:04,646] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['rustiek', 'ongehuwd', 'glamoureus', 'zilvergrijs', 'platinablond', 'erotisch', 'beeldig', 'tuttig', 'levenslustig', 'kinderloos']
[2025-04-27 16:58:04,646] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['blond', 'teder', 'levenslustig', 'beeldig', 'zilvergrijs', 'rozig', 'huwelijks', 'voorlijk', 'ongepland', 'tuttig']
[2025-04-27 16:58:04,646] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['ongehuwd', 'blond', 'teder', 'tenger', 'platinablond', 'beeldig', 'marokkaans', 'levenslustig', 'glamoureus', 'lesbisch']
[2025-04-27 16:58:04,646] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['ongepland', 'beeldschoon', 'levenslustig', 'beeldig', 'teder', 'glamoureus', 'bloedmooie', 'tuttig', 'stijlvol', 'marokkaans']
[2025-04-27 16:58:04,647] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['ongehuwd', 'hitsig', 'spichtig', 'exotisch', 'zilvergrijs', 'ongepland', 'erotisch', 'rimpelig', 'huwelijks', 'beeldschoon']
[2025-04-27 16:58:04,647] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama2-uncensored' with temperature=1.5 and seed=47
[2025-04-27 16:58:04,647] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:58:04,653] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama2-uncensored' with temperature=1.5 and seed=48
[2025-04-27 16:58:04,653] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:58:04,655] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama2-uncensored' with temperature=1.5 and seed=49
[2025-04-27 16:58:04,655] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:58:04,656] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama2-uncensored' with temperature=1.5 and seed=50
[2025-04-27 16:58:04,656] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:58:04,658] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama2-uncensored' with temperature=1.5 and seed=51
[2025-04-27 16:58:04,658] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 16:58:04,659] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,660] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:58:04,660] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,660] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:58:04,660] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:58:04,661] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,662] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 16:58:04,663] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6c980>
[2025-04-27 16:58:04,663] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,663] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6ed50>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6cc20>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,664] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:58:04,665] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:58:04,665] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 16:58:04,665] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 16:58:04,665] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 16:58:04,665] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 16:59:28,070] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 14:59:28 GMT'), (b'Content-Length', b'998')])
[2025-04-27 16:59:28,072] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 16:59:28,073] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 16:59:28,073] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 16:59:28,073] DEBUG: _trace:85 - response_closed.started
[2025-04-27 16:59:28,073] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 16:59:28,074] DEBUG: 01_async_generate_sentences:360 - [Batch 2] Model response time: 83.42 seconds
[2025-04-27 16:59:28,074] ERROR: 01_async_generate_sentences:382 - [Batch 2] Expected exactly 10 sentences, but got 2
[2025-04-27 17:02:10,125] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:02:10 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 17:02:10,127] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:02:10,127] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:02:10,128] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:02:10,128] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:02:10,129] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:02:10,130] DEBUG: _trace:85 - close.started
[2025-04-27 17:02:10,130] DEBUG: _trace:85 - close.complete
[2025-04-27 17:02:10,131] DEBUG: 01_async_generate_sentences:360 - [Batch 0] Model response time: 245.48 seconds
[2025-04-27 17:02:10,132] ERROR: 01_async_generate_sentences:382 - [Batch 0] Expected exactly 10 sentences, but got 9
[2025-04-27 17:02:32,047] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:02:32 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 17:02:32,047] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:02:32,048] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:02:32,048] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:02:32,048] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:02:32,048] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:02:32,048] DEBUG: _trace:85 - close.started
[2025-04-27 17:02:32,049] DEBUG: _trace:85 - close.complete
[2025-04-27 17:02:32,049] DEBUG: 01_async_generate_sentences:360 - [Batch 1] Model response time: 267.40 seconds
[2025-04-27 17:02:32,049] ERROR: 01_async_generate_sentences:382 - [Batch 1] Expected exactly 10 sentences, but got 9
[2025-04-27 17:03:13,727] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:03:13 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 17:03:13,728] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:03:13,728] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:03:13,729] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:03:13,729] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:03:13,729] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:03:13,730] DEBUG: _trace:85 - close.started
[2025-04-27 17:03:13,730] DEBUG: _trace:85 - close.complete
[2025-04-27 17:03:13,730] DEBUG: 01_async_generate_sentences:360 - [Batch 3] Model response time: 309.07 seconds
[2025-04-27 17:03:13,732] ERROR: 01_async_generate_sentences:382 - [Batch 3] Expected exactly 10 sentences, but got 9
[2025-04-27 17:03:25,909] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:03:25 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 17:03:25,910] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:03:25,910] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,910] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:03:25,910] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:03:25,911] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:03:25,911] DEBUG: _trace:85 - close.started
[2025-04-27 17:03:25,911] DEBUG: _trace:85 - close.complete
[2025-04-27 17:03:25,912] DEBUG: 01_async_generate_sentences:360 - [Batch 4] Model response time: 321.25 seconds
[2025-04-27 17:03:25,912] INFO: 01_async_generate_sentences:384 - [Batch 4] Successfully parsed 10 sentences from model 'llama2-uncensored'
[2025-04-27 17:03:25,912] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 9']
[2025-04-27 17:03:25,913] WARNING: 01_async_generate_sentences:177 - {
  "unexpected_word_count": 9,
  "unique_unexpected_words": [
    "bezig",
    "elegant",
    "schoon",
    "zacht",
    "wittig",
    "dapper",
    "moedig",
    "romantiek",
    "fairy tale"
  ],
  "note": "9 unique unexpected words"
}
[2025-04-27 17:03:25,916] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 9']
[2025-04-27 17:03:25,916] WARNING: 01_async_generate_sentences:177 - {
  "unexpected_word_count": 5,
  "unique_unexpected_words": [
    "silvergrijs",
    "tender",
    "rosig",
    "exact"
  ],
  "note": "4 unique unexpected words"
}
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:202 - Added 4 new sentences; Total sentences: 21/200
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 2, 'beeldschoon': 1, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 2, 'feministisch': 0, 'stijlvol': 3, 'tuttig': 1, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 1, 'erotisch': 0, 'kleurig': 1, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 1, 'hitsig': 1, 'rustiek': 0, 'teder': 0, 'marokkaans': 0, 'tenger': 1, 'exotisch': 0}
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 17:03:25,917] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 2']
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:202 - Added 2 new sentences; Total sentences: 23/200
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 3, 'beeldschoon': 1, 'ongepland': 0, 'bloedmooie': 0, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 2, 'feministisch': 0, 'stijlvol': 3, 'tuttig': 1, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 1, 'erotisch': 0, 'kleurig': 1, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 1, 'hitsig': 1, 'rustiek': 0, 'teder': 0, 'marokkaans': 1, 'tenger': 1, 'exotisch': 0}
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 17:03:25,917] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['Expected exactly 10 sentences, but got 9']
[2025-04-27 17:03:25,917] WARNING: 01_async_generate_sentences:177 - {
  "unexpected_word_count": 4,
  "unique_unexpected_words": [
    "geagreep",
    "beleefd",
    "behandsmatig",
    "belegd"
  ],
  "note": "4 unique unexpected words"
}
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:202 - Added 5 new sentences; Total sentences: 28/200
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 3, 'beeldschoon': 1, 'ongepland': 0, 'bloedmooie': 1, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 2, 'feministisch': 0, 'stijlvol': 4, 'tuttig': 2, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 1, 'erotisch': 0, 'kleurig': 1, 'zilvergrijs': 0, 'rozig': 0, 'spichtig': 1, 'levenslustig': 2, 'hitsig': 1, 'rustiek': 0, 'teder': 0, 'marokkaans': 2, 'tenger': 1, 'exotisch': 0}
[2025-04-27 17:03:25,917] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:173 - Skipped 1 duplicate sentences in this batch.
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:202 - Added 9 new sentences; Total sentences: 37/200
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:203 - Current word counts: {'lesbisch': 0, 'blond': 3, 'beeldschoon': 2, 'ongepland': 0, 'bloedmooie': 1, 'beeldig': 1, 'sensueel': 1, 'platinablond': 1, 'voorlijk': 2, 'feministisch': 0, 'stijlvol': 4, 'tuttig': 2, 'huwelijks': 1, 'donkerharig': 1, 'ongehuwd': 0, 'kinderloos': 0, 'glamoureus': 1, 'rimpelig': 2, 'erotisch': 3, 'kleurig': 1, 'zilvergrijs': 1, 'rozig': 0, 'spichtig': 2, 'levenslustig': 2, 'hitsig': 2, 'rustiek': 0, 'teder': 0, 'marokkaans': 2, 'tenger': 1, 'exotisch': 1}
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:204 - Words still available: 30
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:557 - Collected 37 sentences after 477.6 seconds
[2025-04-27 17:03:25,918] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 17:03:25,919] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['spichtig', 'zilvergrijs', 'rozig', 'sensueel', 'teder', 'platinablond', 'glamoureus', 'donkerharig', 'levenslustig', 'ongepland']
[2025-04-27 17:03:25,919] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['kinderloos', 'exotisch', 'erotisch', 'ongehuwd', 'rustiek', 'voorlijk', 'rozig', 'tuttig', 'kleurig', 'tenger']
[2025-04-27 17:03:25,919] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['platinablond', 'rozig', 'donkerharig', 'beeldschoon', 'lesbisch', 'glamoureus', 'stijlvol', 'bloedmooie', 'beeldig', 'marokkaans']
[2025-04-27 17:03:25,919] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['spichtig', 'bloedmooie', 'hitsig', 'feministisch', 'voorlijk', 'teder', 'kleurig', 'rozig', 'sensueel', 'platinablond']
[2025-04-27 17:03:25,919] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['marokkaans', 'erotisch', 'hitsig', 'bloedmooie', 'feministisch', 'beeldschoon', 'ongepland', 'stijlvol', 'exotisch', 'blond']
[2025-04-27 17:03:25,920] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama2-uncensored' with temperature=1.5 and seed=52
[2025-04-27 17:03:25,920] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:03:25,925] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama2-uncensored' with temperature=1.5 and seed=53
[2025-04-27 17:03:25,925] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:03:25,927] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama2-uncensored' with temperature=1.5 and seed=54
[2025-04-27 17:03:25,927] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:03:25,929] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama2-uncensored' with temperature=1.5 and seed=55
[2025-04-27 17:03:25,929] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:03:25,930] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama2-uncensored' with temperature=1.5 and seed=56
[2025-04-27 17:03:25,930] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:03:25,931] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,931] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:03:25,931] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,931] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:03:25,932] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:03:25,932] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,932] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:03:25,933] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:03:25,933] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6d520>
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6faa0>
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,934] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6eed0>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103d6fb90>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:03:25,935] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:03:25,936] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:03:25,936] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:03:25,936] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:03:25,936] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:04:28,205] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:04:28 GMT'), (b'Content-Length', b'596')])
[2025-04-27 17:04:28,208] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:04:28,208] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:04:28,209] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:04:28,209] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:04:28,209] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:04:28,211] DEBUG: 01_async_generate_sentences:360 - [Batch 2] Model response time: 62.28 seconds
[2025-04-27 17:04:28,211] ERROR: 01_async_generate_sentences:382 - [Batch 2] Expected exactly 10 sentences, but got 1
[2025-04-27 17:05:06,789] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:05:06 GMT'), (b'Content-Length', b'590')])
[2025-04-27 17:05:06,790] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:05:06,790] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:05:06,790] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:05:06,790] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:05:06,791] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:05:06,791] DEBUG: _trace:85 - close.started
[2025-04-27 17:05:06,791] DEBUG: _trace:85 - close.complete
[2025-04-27 17:05:06,792] DEBUG: 01_async_generate_sentences:360 - [Batch 3] Model response time: 100.86 seconds
[2025-04-27 17:05:06,792] ERROR: 01_async_generate_sentences:382 - [Batch 3] Expected exactly 10 sentences, but got 2
[2025-04-27 17:06:17,845] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:06:17 GMT'), (b'Content-Length', b'1739')])
[2025-04-27 17:06:17,848] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
[2025-04-27 17:06:17,848] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:06:17,848] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:06:17,848] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:06:17,849] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:06:17,849] DEBUG: _trace:85 - close.started
[2025-04-27 17:06:17,849] DEBUG: _trace:85 - close.complete
[2025-04-27 17:06:17,850] DEBUG: 01_async_generate_sentences:360 - [Batch 1] Model response time: 171.92 seconds
[2025-04-27 17:06:17,851] ERROR: 01_async_generate_sentences:382 - [Batch 1] Expected exactly 10 sentences, but got 5
[2025-04-27 17:07:02,006] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:07:02 GMT'), (b'Transfer-Encoding', b'chunked')])
[2025-04-27 17:40:03,990] DEBUG: 01_async_generate_sentences:72 - Logger initialized with async processing. Script started.
[2025-04-27 17:40:03,991] DEBUG: selector_events:64 - Using selector: KqueueSelector
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:632 - ===== EXPERIMENT START =====
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:633 - Processing with targets: 200 total sentences, max 15 per word
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:634 - Using models: ['llama3-chatqa:8b', 'llama3:text', 'llama3:8b', 'llama2-uncensored'] with temperatures: [0.5, 0.75, 1, 1.25, 1.5]
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:645 - Found 4 prompt files: ['prompt_femaleNouns_femaleAdjs.txt', 'prompt_femaleNouns_maleAdjs.txt', 'prompt_maleNouns_maleAdjs.txt', 'prompt_maleNouns_femaleAdjs.txt']
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:649 - ===== PROCESSING prompt_femaleNouns_femaleAdjs.txt =====
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:501 - Processing file prompt_femaleNouns_femaleAdjs.txt: noun_gender: female, adjective_gender: female
[2025-04-27 17:40:03,991] INFO: 01_async_generate_sentences:511 - Successfully read the prompt from /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/prompts/prompt_femaleNouns_femaleAdjs.txt
[2025-04-27 17:40:03,991] DEBUG: _config:82 - load_ssl_context verify=True cert=None trust_env=True http2=False
[2025-04-27 17:40:03,992] DEBUG: _config:148 - load_verify_locations cafile='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/certifi/cacert.pem'
[2025-04-27 17:40:04,008] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama3-chatqa:8b': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b
[2025-04-27 17:40:04,008] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,008] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,008] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['exotisch', 'sensueel', 'kinderloos', 'ongepland', 'kleurig', 'tuttig', 'platinablond', 'rimpelig', 'blond', 'rozig']
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['beeldig', 'ongepland', 'rustiek', 'erotisch', 'bloedmooie', 'sensueel', 'donkerharig', 'kleurig', 'glamoureus', 'huwelijks']
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['exotisch', 'rustiek', 'rozig', 'glamoureus', 'zilvergrijs', 'voorlijk', 'ongehuwd', 'bloedmooie', 'feministisch', 'beeldschoon']
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['sensueel', 'donkerharig', 'levenslustig', 'kleurig', 'tuttig', 'feministisch', 'zilvergrijs', 'marokkaans', 'lesbisch', 'beeldig']
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['zilvergrijs', 'beeldschoon', 'hitsig', 'bloedmooie', 'ongehuwd', 'rozig', 'marokkaans', 'ongepland', 'feministisch', 'kleurig']
[2025-04-27 17:40:04,008] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=42
[2025-04-27 17:40:04,008] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,013] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=43
[2025-04-27 17:40:04,013] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,014] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=44
[2025-04-27 17:40:04,014] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,014] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=45
[2025-04-27 17:40:04,014] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,014] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=46
[2025-04-27 17:40:04,014] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,015] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:04,015] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:04,015] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:04,015] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:04,015] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1071efd70>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x107368ec0>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x107368fb0>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1073699d0>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10736a3c0>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,016] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,017] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,031] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,032] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,032] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,032] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,032] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,033] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,033] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,033] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,033] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,033] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,033] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,034] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,034] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,034] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,034] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,034] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,034] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,034] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,034] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,034] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,034] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['bloedmooie', 'ongepland', 'beeldschoon', 'lesbisch', 'marokkaans', 'beeldig', 'kinderloos', 'platinablond', 'rimpelig', 'kleurig']
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['kinderloos', 'rozig', 'platinablond', 'feministisch', 'hitsig', 'glamoureus', 'beeldschoon', 'kleurig', 'rustiek', 'lesbisch']
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['tenger', 'voorlijk', 'ongehuwd', 'levenslustig', 'beeldig', 'kinderloos', 'erotisch', 'blond', 'rozig', 'huwelijks']
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['spichtig', 'sensueel', 'stijlvol', 'rimpelig', 'zilvergrijs', 'voorlijk', 'beeldschoon', 'levenslustig', 'feministisch', 'erotisch']
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['tuttig', 'spichtig', 'blond', 'voorlijk', 'ongehuwd', 'bloedmooie', 'erotisch', 'beeldig', 'ongepland', 'lesbisch']
[2025-04-27 17:40:04,034] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=47
[2025-04-27 17:40:04,034] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,035] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=48
[2025-04-27 17:40:04,035] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,036] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=49
[2025-04-27 17:40:04,036] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,036] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=50
[2025-04-27 17:40:04,036] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,037] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=51
[2025-04-27 17:40:04,037] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,037] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,038] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,039] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,039] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,040] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,040] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,040] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,040] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,041] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,041] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,041] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,041] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,041] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,042] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,042] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,042] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,042] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,042] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,042] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,042] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,042] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,042] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,042] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,042] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,042] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,042] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,042] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['erotisch', 'donkerharig', 'voorlijk', 'rimpelig', 'teder', 'beeldig', 'exotisch', 'tenger', 'glamoureus', 'rozig']
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['ongehuwd', 'blond', 'rimpelig', 'glamoureus', 'kinderloos', 'rustiek', 'teder', 'kleurig', 'ongepland', 'beeldig']
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['stijlvol', 'huwelijks', 'glamoureus', 'donkerharig', 'feministisch', 'teder', 'voorlijk', 'rozig', 'bloedmooie', 'beeldig']
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['voorlijk', 'lesbisch', 'platinablond', 'marokkaans', 'feministisch', 'hitsig', 'ongehuwd', 'rustiek', 'blond', 'donkerharig']
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['rimpelig', 'erotisch', 'spichtig', 'kleurig', 'voorlijk', 'stijlvol', 'hitsig', 'levenslustig', 'sensueel', 'tenger']
[2025-04-27 17:40:04,042] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=52
[2025-04-27 17:40:04,042] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,043] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=53
[2025-04-27 17:40:04,043] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,043] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=54
[2025-04-27 17:40:04,044] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,044] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=55
[2025-04-27 17:40:04,044] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,045] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=56
[2025-04-27 17:40:04,045] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,045] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,045] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,045] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,046] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,047] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,047] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,047] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,048] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,048] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,048] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,048] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,049] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,049] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,049] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,049] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,049] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,050] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,050] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,050] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,050] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,050] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,050] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,050] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,050] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['spichtig', 'rozig', 'kinderloos', 'beeldschoon', 'erotisch', 'tenger', 'bloedmooie', 'tuttig', 'levenslustig', 'glamoureus']
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['beeldig', 'teder', 'ongepland', 'bloedmooie', 'donkerharig', 'kleurig', 'marokkaans', 'ongehuwd', 'rozig', 'platinablond']
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['lesbisch', 'ongehuwd', 'zilvergrijs', 'donkerharig', 'huwelijks', 'ongepland', 'levenslustig', 'rustiek', 'sensueel', 'voorlijk']
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['spichtig', 'hitsig', 'platinablond', 'ongehuwd', 'kinderloos', 'teder', 'donkerharig', 'rustiek', 'exotisch', 'tuttig']
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['glamoureus', 'ongepland', 'teder', 'platinablond', 'kleurig', 'hitsig', 'rimpelig', 'sensueel', 'tuttig', 'lesbisch']
[2025-04-27 17:40:04,050] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=57
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,050] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=58
[2025-04-27 17:40:04,050] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,051] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=59
[2025-04-27 17:40:04,051] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,051] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=60
[2025-04-27 17:40:04,051] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,052] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=61
[2025-04-27 17:40:04,052] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,052] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,053] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,054] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,054] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,054] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,055] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,055] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,055] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,055] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,055] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,056] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,056] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,056] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,056] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,056] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,056] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,056] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:04,057] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_5_sentences_1745768404.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_5_state_1745768404.json
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp0.5.jsonl for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['lesbisch', 'huwelijks', 'voorlijk', 'rimpelig', 'kleurig', 'ongehuwd', 'stijlvol', 'hitsig', 'rozig', 'feministisch']
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['levenslustig', 'rimpelig', 'rozig', 'kinderloos', 'ongepland', 'feministisch', 'tenger', 'beeldig', 'marokkaans', 'zilvergrijs']
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['beeldschoon', 'platinablond', 'teder', 'kleurig', 'hitsig', 'stijlvol', 'exotisch', 'blond', 'voorlijk', 'tuttig']
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['tenger', 'bloedmooie', 'marokkaans', 'hitsig', 'feministisch', 'spichtig', 'levenslustig', 'ongepland', 'erotisch', 'rimpelig']
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['ongehuwd', 'stijlvol', 'zilvergrijs', 'erotisch', 'tuttig', 'glamoureus', 'kleurig', 'rustiek', 'blond', 'rozig']
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=42
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,058] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=43
[2025-04-27 17:40:04,058] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,059] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=44
[2025-04-27 17:40:04,059] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,059] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=45
[2025-04-27 17:40:04,059] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,060] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=46
[2025-04-27 17:40:04,060] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,060] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,061] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,062] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,062] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,062] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,062] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,063] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,063] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,063] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,063] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,063] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,063] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,064] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,064] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,064] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,064] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,064] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,064] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,064] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,064] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,064] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,064] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,064] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,064] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,064] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,064] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['zilvergrijs', 'ongehuwd', 'lesbisch', 'teder', 'spichtig', 'tenger', 'kinderloos', 'erotisch', 'ongepland', 'blond']
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['bloedmooie', 'hitsig', 'tuttig', 'lesbisch', 'ongepland', 'platinablond', 'tenger', 'zilvergrijs', 'ongehuwd', 'teder']
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['erotisch', 'bloedmooie', 'kinderloos', 'ongepland', 'sensueel', 'hitsig', 'tenger', 'spichtig', 'huwelijks', 'blond']
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['ongepland', 'marokkaans', 'platinablond', 'exotisch', 'lesbisch', 'ongehuwd', 'spichtig', 'blond', 'hitsig', 'sensueel']
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['kleurig', 'spichtig', 'lesbisch', 'beeldschoon', 'glamoureus', 'rustiek', 'voorlijk', 'huwelijks', 'tenger', 'exotisch']
[2025-04-27 17:40:04,064] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=47
[2025-04-27 17:40:04,064] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,065] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=48
[2025-04-27 17:40:04,065] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,065] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=49
[2025-04-27 17:40:04,065] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,066] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=50
[2025-04-27 17:40:04,066] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,066] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=51
[2025-04-27 17:40:04,066] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,067] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,068] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,068] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,069] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,069] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,069] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,069] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,069] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,070] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,070] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,070] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,070] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,070] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,070] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,070] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,070] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,071] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,071] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,071] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,071] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,071] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,071] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,071] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,071] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,071] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,071] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,071] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,071] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,071] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,071] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['huwelijks', 'sensueel', 'zilvergrijs', 'rustiek', 'lesbisch', 'stijlvol', 'marokkaans', 'teder', 'erotisch', 'glamoureus']
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['voorlijk', 'bloedmooie', 'hitsig', 'donkerharig', 'platinablond', 'spichtig', 'exotisch', 'ongehuwd', 'rimpelig', 'huwelijks']
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['rozig', 'huwelijks', 'tuttig', 'tenger', 'hitsig', 'stijlvol', 'platinablond', 'kleurig', 'glamoureus', 'rimpelig']
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['zilvergrijs', 'erotisch', 'rimpelig', 'donkerharig', 'beeldig', 'teder', 'tuttig', 'platinablond', 'bloedmooie', 'ongepland']
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['platinablond', 'spichtig', 'kleurig', 'huwelijks', 'ongehuwd', 'ongepland', 'kinderloos', 'sensueel', 'erotisch', 'exotisch']
[2025-04-27 17:40:04,071] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=52
[2025-04-27 17:40:04,071] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,072] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=53
[2025-04-27 17:40:04,072] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,072] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=54
[2025-04-27 17:40:04,072] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,073] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=55
[2025-04-27 17:40:04,073] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,074] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=56
[2025-04-27 17:40:04,074] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,074] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,075] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,076] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,076] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,076] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,076] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,077] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,077] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,077] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,077] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,077] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,078] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,078] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,078] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,078] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,079] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,079] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,079] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,079] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,079] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,079] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,079] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,079] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,079] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,079] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,079] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,079] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['ongehuwd', 'levenslustig', 'exotisch', 'rustiek', 'ongepland', 'tuttig', 'kinderloos', 'sensueel', 'beeldschoon', 'erotisch']
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['beeldschoon', 'marokkaans', 'voorlijk', 'blond', 'rozig', 'erotisch', 'stijlvol', 'rustiek', 'ongehuwd', 'teder']
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['stijlvol', 'rozig', 'platinablond', 'sensueel', 'kinderloos', 'hitsig', 'kleurig', 'zilvergrijs', 'levenslustig', 'feministisch']
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['kinderloos', 'marokkaans', 'rimpelig', 'donkerharig', 'voorlijk', 'hitsig', 'levenslustig', 'teder', 'tenger', 'bloedmooie']
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['zilvergrijs', 'stijlvol', 'kinderloos', 'beeldig', 'ongepland', 'beeldschoon', 'hitsig', 'rustiek', 'tenger', 'erotisch']
[2025-04-27 17:40:04,079] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=57
[2025-04-27 17:40:04,079] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,080] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=58
[2025-04-27 17:40:04,080] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,080] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=59
[2025-04-27 17:40:04,080] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,080] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=60
[2025-04-27 17:40:04,080] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,081] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=61
[2025-04-27 17:40:04,081] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,082] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,083] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,083] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,084] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,084] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,084] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,084] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,084] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,085] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,085] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,085] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,085] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,085] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,086] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,086] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,086] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:04,086] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_75_sentences_1745768404.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_75_state_1745768404.json
[2025-04-27 17:40:04,086] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp0.75.jsonl for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:04,087] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,087] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,087] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['spichtig', 'marokkaans', 'glamoureus', 'rozig', 'ongehuwd', 'bloedmooie', 'feministisch', 'platinablond', 'levenslustig', 'kleurig']
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['kleurig', 'kinderloos', 'donkerharig', 'levenslustig', 'ongehuwd', 'rozig', 'zilvergrijs', 'voorlijk', 'stijlvol', 'glamoureus']
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['rozig', 'kinderloos', 'glamoureus', 'stijlvol', 'zilvergrijs', 'tuttig', 'sensueel', 'erotisch', 'spichtig', 'levenslustig']
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['tuttig', 'marokkaans', 'voorlijk', 'rimpelig', 'kinderloos', 'beeldig', 'donkerharig', 'stijlvol', 'blond', 'erotisch']
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['marokkaans', 'kinderloos', 'sensueel', 'ongepland', 'rustiek', 'platinablond', 'rimpelig', 'teder', 'glamoureus', 'levenslustig']
[2025-04-27 17:40:04,087] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=42
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,087] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=43
[2025-04-27 17:40:04,087] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,088] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=44
[2025-04-27 17:40:04,088] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,088] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=45
[2025-04-27 17:40:04,088] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,089] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=46
[2025-04-27 17:40:04,089] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,089] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,090] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,091] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,091] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,091] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,091] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,092] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,092] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,092] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,092] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,092] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,093] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,093] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,093] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,093] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,093] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,093] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,093] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,093] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,093] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,093] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,093] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,093] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,093] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,093] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,093] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['blond', 'ongepland', 'spichtig', 'rozig', 'rustiek', 'stijlvol', 'hitsig', 'voorlijk', 'kinderloos', 'zilvergrijs']
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['zilvergrijs', 'hitsig', 'stijlvol', 'sensueel', 'rustiek', 'kleurig', 'tenger', 'ongehuwd', 'ongepland', 'beeldig']
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['huwelijks', 'kleurig', 'exotisch', 'erotisch', 'levenslustig', 'beeldig', 'lesbisch', 'platinablond', 'zilvergrijs', 'bloedmooie']
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['ongehuwd', 'ongepland', 'rozig', 'zilvergrijs', 'voorlijk', 'beeldschoon', 'beeldig', 'teder', 'hitsig', 'lesbisch']
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['hitsig', 'zilvergrijs', 'lesbisch', 'stijlvol', 'tuttig', 'glamoureus', 'rimpelig', 'tenger', 'spichtig', 'levenslustig']
[2025-04-27 17:40:04,093] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=47
[2025-04-27 17:40:04,093] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,094] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=48
[2025-04-27 17:40:04,094] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,094] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=49
[2025-04-27 17:40:04,094] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,095] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=50
[2025-04-27 17:40:04,095] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,095] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=51
[2025-04-27 17:40:04,095] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,096] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,097] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,097] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,098] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,098] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,098] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,099] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,099] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,099] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,099] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,099] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,099] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,100] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,100] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,100] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,100] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,100] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,100] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,100] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,100] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,100] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,100] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,100] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,100] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,100] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,100] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['donkerharig', 'bloedmooie', 'ongehuwd', 'rustiek', 'levenslustig', 'exotisch', 'tuttig', 'tenger', 'zilvergrijs', 'lesbisch']
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['ongehuwd', 'tenger', 'blond', 'feministisch', 'spichtig', 'marokkaans', 'beeldig', 'sensueel', 'kleurig', 'tuttig']
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['platinablond', 'ongepland', 'erotisch', 'stijlvol', 'glamoureus', 'bloedmooie', 'levenslustig', 'beeldschoon', 'blond', 'tenger']
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['bloedmooie', 'spichtig', 'marokkaans', 'levenslustig', 'teder', 'zilvergrijs', 'kleurig', 'kinderloos', 'hitsig', 'blond']
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['blond', 'feministisch', 'spichtig', 'tuttig', 'erotisch', 'glamoureus', 'rozig', 'voorlijk', 'ongehuwd', 'zilvergrijs']
[2025-04-27 17:40:04,100] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=52
[2025-04-27 17:40:04,100] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,101] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=53
[2025-04-27 17:40:04,101] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,101] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=54
[2025-04-27 17:40:04,101] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,102] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=55
[2025-04-27 17:40:04,102] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,102] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=56
[2025-04-27 17:40:04,102] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,103] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,104] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,104] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,104] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,105] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,105] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,105] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,105] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,105] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,106] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,106] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,106] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,106] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,106] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,107] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,107] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,107] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,107] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,107] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,107] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,107] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['erotisch', 'spichtig', 'ongepland', 'blond', 'beeldig', 'kleurig', 'kinderloos', 'tenger', 'platinablond', 'lesbisch']
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['zilvergrijs', 'rustiek', 'huwelijks', 'blond', 'voorlijk', 'rimpelig', 'ongepland', 'tenger', 'beeldschoon', 'feministisch']
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['blond', 'lesbisch', 'zilvergrijs', 'rimpelig', 'bloedmooie', 'spichtig', 'donkerharig', 'sensueel', 'marokkaans', 'glamoureus']
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['marokkaans', 'tenger', 'beeldig', 'sensueel', 'beeldschoon', 'teder', 'ongepland', 'donkerharig', 'erotisch', 'bloedmooie']
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['glamoureus', 'ongehuwd', 'rozig', 'hitsig', 'bloedmooie', 'feministisch', 'beeldschoon', 'spichtig', 'huwelijks', 'blond']
[2025-04-27 17:40:04,107] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=57
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,107] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=58
[2025-04-27 17:40:04,107] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,108] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=59
[2025-04-27 17:40:04,108] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,108] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=60
[2025-04-27 17:40:04,108] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,109] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=61
[2025-04-27 17:40:04,109] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,109] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,109] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,109] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,109] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,110] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,110] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,111] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,111] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,111] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,111] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,112] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,112] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,112] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,112] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,112] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,113] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,113] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,113] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,113] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,113] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,113] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,113] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,113] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,113] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:04,113] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_sentences_1745768404.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_state_1745768404.json
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.jsonl for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['ongepland', 'donkerharig', 'hitsig', 'exotisch', 'rustiek', 'huwelijks', 'voorlijk', 'beeldig', 'rimpelig', 'platinablond']
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['huwelijks', 'lesbisch', 'rimpelig', 'hitsig', 'ongepland', 'stijlvol', 'erotisch', 'zilvergrijs', 'teder', 'marokkaans']
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['rozig', 'feministisch', 'beeldig', 'rustiek', 'ongepland', 'marokkaans', 'voorlijk', 'rimpelig', 'tenger', 'platinablond']
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['kleurig', 'hitsig', 'exotisch', 'feministisch', 'levenslustig', 'rimpelig', 'ongehuwd', 'tenger', 'voorlijk', 'rozig']
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['erotisch', 'rustiek', 'stijlvol', 'ongehuwd', 'tuttig', 'feministisch', 'voorlijk', 'huwelijks', 'exotisch', 'donkerharig']
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=42
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,114] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=43
[2025-04-27 17:40:04,114] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,115] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=44
[2025-04-27 17:40:04,115] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,115] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=45
[2025-04-27 17:40:04,115] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,116] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=46
[2025-04-27 17:40:04,116] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,116] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,117] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,118] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,118] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,118] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,118] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,119] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,119] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,119] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,119] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,119] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,120] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,120] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,120] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,120] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,120] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,120] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,120] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,120] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,120] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,120] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,120] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['blond', 'spichtig', 'feministisch', 'tuttig', 'donkerharig', 'platinablond', 'rozig', 'voorlijk', 'huwelijks', 'marokkaans']
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['ongehuwd', 'hitsig', 'beeldschoon', 'feministisch', 'glamoureus', 'huwelijks', 'sensueel', 'rustiek', 'blond', 'tenger']
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['voorlijk', 'beeldschoon', 'zilvergrijs', 'spichtig', 'teder', 'feministisch', 'erotisch', 'kleurig', 'bloedmooie', 'exotisch']
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['tenger', 'rustiek', 'spichtig', 'platinablond', 'marokkaans', 'ongepland', 'rozig', 'blond', 'sensueel', 'bloedmooie']
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['rustiek', 'tuttig', 'beeldig', 'lesbisch', 'exotisch', 'ongepland', 'sensueel', 'marokkaans', 'beeldschoon', 'feministisch']
[2025-04-27 17:40:04,120] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=47
[2025-04-27 17:40:04,120] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,121] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=48
[2025-04-27 17:40:04,121] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,121] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=49
[2025-04-27 17:40:04,121] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,122] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=50
[2025-04-27 17:40:04,122] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,122] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=51
[2025-04-27 17:40:04,122] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,123] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,124] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,124] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,125] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,125] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,125] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,125] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,125] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,126] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,126] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,126] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,126] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,126] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,127] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,127] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,127] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,127] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,127] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,127] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,127] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,127] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['tenger', 'rozig', 'beeldschoon', 'tuttig', 'voorlijk', 'spichtig', 'feministisch', 'erotisch', 'zilvergrijs', 'donkerharig']
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['kleurig', 'sensueel', 'ongehuwd', 'donkerharig', 'spichtig', 'lesbisch', 'blond', 'hitsig', 'beeldig', 'voorlijk']
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['exotisch', 'kinderloos', 'hitsig', 'beeldschoon', 'teder', 'rustiek', 'marokkaans', 'tuttig', 'donkerharig', 'sensueel']
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['tenger', 'rustiek', 'marokkaans', 'donkerharig', 'spichtig', 'feministisch', 'bloedmooie', 'exotisch', 'ongepland', 'lesbisch']
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['teder', 'donkerharig', 'spichtig', 'huwelijks', 'glamoureus', 'platinablond', 'sensueel', 'erotisch', 'tenger', 'kleurig']
[2025-04-27 17:40:04,127] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=52
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,127] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=53
[2025-04-27 17:40:04,127] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,128] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=54
[2025-04-27 17:40:04,128] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,128] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=55
[2025-04-27 17:40:04,128] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,129] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=56
[2025-04-27 17:40:04,129] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,129] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,129] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,129] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,130] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,131] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,131] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,131] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,131] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,132] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,132] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,132] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,132] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,132] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,132] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,133] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,133] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,133] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,133] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,133] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,133] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,133] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,133] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,133] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,133] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,133] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,133] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,133] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,133] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['spichtig', 'bloedmooie', 'hitsig', 'platinablond', 'exotisch', 'beeldig', 'donkerharig', 'sensueel', 'blond', 'stijlvol']
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['beeldschoon', 'erotisch', 'tuttig', 'lesbisch', 'spichtig', 'glamoureus', 'donkerharig', 'zilvergrijs', 'ongepland', 'beeldig']
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['donkerharig', 'stijlvol', 'blond', 'exotisch', 'rimpelig', 'marokkaans', 'erotisch', 'voorlijk', 'zilvergrijs', 'teder']
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['erotisch', 'kleurig', 'hitsig', 'bloedmooie', 'feministisch', 'beeldig', 'voorlijk', 'glamoureus', 'rozig', 'marokkaans']
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['huwelijks', 'rimpelig', 'ongepland', 'tenger', 'kinderloos', 'feministisch', 'donkerharig', 'lesbisch', 'beeldig', 'zilvergrijs']
[2025-04-27 17:40:04,133] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=57
[2025-04-27 17:40:04,133] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,134] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=58
[2025-04-27 17:40:04,134] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,134] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=59
[2025-04-27 17:40:04,134] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,135] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=60
[2025-04-27 17:40:04,135] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,135] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=61
[2025-04-27 17:40:04,135] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,136] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,137] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,137] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,137] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,138] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,138] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,138] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,138] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,138] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,139] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,139] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,139] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,139] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,139] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,139] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,139] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_25_sentences_1745768404.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_25_state_1745768404.json
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.25.jsonl for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['spichtig', 'zilvergrijs', 'exotisch', 'levenslustig', 'donkerharig', 'teder', 'voorlijk', 'marokkaans', 'stijlvol', 'tenger']
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['tuttig', 'bloedmooie', 'ongepland', 'rimpelig', 'hitsig', 'levenslustig', 'kinderloos', 'huwelijks', 'stijlvol', 'glamoureus']
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['levenslustig', 'kinderloos', 'ongehuwd', 'platinablond', 'glamoureus', 'rimpelig', 'blond', 'marokkaans', 'feministisch', 'beeldschoon']
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['platinablond', 'tuttig', 'kinderloos', 'huwelijks', 'voorlijk', 'stijlvol', 'marokkaans', 'rozig', 'ongehuwd', 'lesbisch']
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['blond', 'voorlijk', 'sensueel', 'levenslustig', 'kleurig', 'teder', 'ongepland', 'tenger', 'rustiek', 'zilvergrijs']
[2025-04-27 17:40:04,140] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=42
[2025-04-27 17:40:04,140] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,141] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=43
[2025-04-27 17:40:04,141] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,141] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=44
[2025-04-27 17:40:04,141] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,142] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=45
[2025-04-27 17:40:04,142] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,142] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=46
[2025-04-27 17:40:04,142] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,143] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,144] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,144] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,144] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,145] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,145] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,145] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,145] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,145] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,146] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,146] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,146] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,146] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,146] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,146] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,146] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,146] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,146] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,147] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,147] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,147] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['sensueel', 'beeldig', 'donkerharig', 'teder', 'kinderloos', 'rustiek', 'bloedmooie', 'ongehuwd', 'voorlijk', 'marokkaans']
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['rozig', 'spichtig', 'tuttig', 'levenslustig', 'teder', 'blond', 'kinderloos', 'feministisch', 'lesbisch', 'stijlvol']
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['kinderloos', 'zilvergrijs', 'rustiek', 'huwelijks', 'spichtig', 'levenslustig', 'rozig', 'kleurig', 'blond', 'feministisch']
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['erotisch', 'beeldschoon', 'stijlvol', 'spichtig', 'sensueel', 'teder', 'tuttig', 'lesbisch', 'rozig', 'tenger']
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['voorlijk', 'tuttig', 'stijlvol', 'rustiek', 'spichtig', 'teder', 'kleurig', 'feministisch', 'huwelijks', 'beeldschoon']
[2025-04-27 17:40:04,147] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=47
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,147] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=48
[2025-04-27 17:40:04,147] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,148] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=49
[2025-04-27 17:40:04,148] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,148] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=50
[2025-04-27 17:40:04,148] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,149] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=51
[2025-04-27 17:40:04,149] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,149] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,149] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,149] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,150] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,150] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,151] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,151] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,151] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,151] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,152] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,152] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,152] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,152] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,152] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,153] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,153] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,153] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,153] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,153] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,153] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,153] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,153] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,153] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,153] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,153] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,153] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,153] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['tuttig', 'spichtig', 'hitsig', 'kinderloos', 'rimpelig', 'beeldig', 'feministisch', 'rozig', 'erotisch', 'ongepland']
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['rustiek', 'platinablond', 'levenslustig', 'rimpelig', 'erotisch', 'rozig', 'kleurig', 'spichtig', 'blond', 'marokkaans']
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['ongepland', 'zilvergrijs', 'levenslustig', 'kleurig', 'teder', 'platinablond', 'lesbisch', 'erotisch', 'hitsig', 'rimpelig']
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['spichtig', 'blond', 'rozig', 'ongepland', 'lesbisch', 'huwelijks', 'feministisch', 'voorlijk', 'glamoureus', 'hitsig']
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['glamoureus', 'lesbisch', 'ongehuwd', 'zilvergrijs', 'beeldig', 'huwelijks', 'rustiek', 'rimpelig', 'spichtig', 'sensueel']
[2025-04-27 17:40:04,153] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=52
[2025-04-27 17:40:04,153] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,154] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=53
[2025-04-27 17:40:04,154] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,154] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=54
[2025-04-27 17:40:04,154] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,155] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=55
[2025-04-27 17:40:04,155] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,155] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=56
[2025-04-27 17:40:04,155] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,156] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,157] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,157] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,158] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,158] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,158] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,158] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,158] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,159] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,159] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,159] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,159] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,159] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,160] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,160] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,160] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,160] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,160] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,160] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,160] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,160] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['hitsig', 'bloedmooie', 'stijlvol', 'huwelijks', 'kleurig', 'marokkaans', 'ongehuwd', 'erotisch', 'donkerharig', 'levenslustig']
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['rozig', 'stijlvol', 'sensueel', 'tuttig', 'ongehuwd', 'voorlijk', 'hitsig', 'erotisch', 'bloedmooie', 'kleurig']
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['voorlijk', 'huwelijks', 'lesbisch', 'tenger', 'donkerharig', 'beeldschoon', 'platinablond', 'blond', 'sensueel', 'kinderloos']
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['exotisch', 'tuttig', 'rimpelig', 'huwelijks', 'ongehuwd', 'beeldschoon', 'kleurig', 'platinablond', 'rozig', 'feministisch']
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['hitsig', 'spichtig', 'stijlvol', 'blond', 'huwelijks', 'tuttig', 'beeldig', 'ongepland', 'kleurig', 'sensueel']
[2025-04-27 17:40:04,160] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=57
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,160] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=58
[2025-04-27 17:40:04,160] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,161] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=59
[2025-04-27 17:40:04,161] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,161] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=60
[2025-04-27 17:40:04,161] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,168] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=61
[2025-04-27 17:40:04,168] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,169] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,170] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,170] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,171] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,171] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,171] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,171] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,171] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,172] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,172] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,172] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:04 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:04,172] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:04,172] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:04,173] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:04,173] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:04,173] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,173] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:04,173] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_5_sentences_1745768404.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_5_state_1745768404.json
[2025-04-27 17:40:04,173] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.5.jsonl for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama3:text': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3_text
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3:text' with temperature 0.5
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3:text' with temperature 0.5
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['levenslustig', 'beeldschoon', 'zilvergrijs', 'exotisch', 'kleurig', 'huwelijks', 'ongepland', 'lesbisch', 'teder', 'erotisch']
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['glamoureus', 'rustiek', 'beeldschoon', 'blond', 'kleurig', 'huwelijks', 'beeldig', 'zilvergrijs', 'ongepland', 'feministisch']
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['kinderloos', 'sensueel', 'levenslustig', 'teder', 'tenger', 'marokkaans', 'platinablond', 'blond', 'huwelijks', 'rustiek']
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['kinderloos', 'tuttig', 'kleurig', 'teder', 'rozig', 'ongepland', 'platinablond', 'rimpelig', 'donkerharig', 'stijlvol']
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['donkerharig', 'bloedmooie', 'blond', 'glamoureus', 'levenslustig', 'platinablond', 'erotisch', 'tenger', 'teder', 'stijlvol']
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3:text' with temperature=0.5 and seed=42
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,174] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3:text' with temperature=0.5 and seed=43
[2025-04-27 17:40:04,174] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,175] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3:text' with temperature=0.5 and seed=44
[2025-04-27 17:40:04,175] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,175] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3:text' with temperature=0.5 and seed=45
[2025-04-27 17:40:04,175] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,176] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3:text' with temperature=0.5 and seed=46
[2025-04-27 17:40:04,176] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:04,176] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,176] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,177] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:04,178] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:08,665] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:08,666] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:08,671] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:08,673] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:08,673] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:08,673] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:08,673] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:08,676] INFO: 01_async_generate_sentences:664 - Process interrupted by user. Shutting down gracefully.
[2025-04-27 17:40:09,207] DEBUG: 01_async_generate_sentences:72 - Logger initialized with async processing. Script started.
[2025-04-27 17:40:09,207] DEBUG: selector_events:64 - Using selector: KqueueSelector
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:632 - ===== EXPERIMENT START =====
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:633 - Processing with targets: 200 total sentences, max 15 per word
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:634 - Using models: ['llama3-chatqa:8b', 'llama3:text', 'llama3:8b', 'llama2-uncensored'] with temperatures: [0.5, 0.75, 1, 1.25, 1.5]
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:645 - Found 4 prompt files: ['prompt_femaleNouns_femaleAdjs.txt', 'prompt_femaleNouns_maleAdjs.txt', 'prompt_maleNouns_maleAdjs.txt', 'prompt_maleNouns_femaleAdjs.txt']
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:649 - ===== PROCESSING prompt_femaleNouns_femaleAdjs.txt =====
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:501 - Processing file prompt_femaleNouns_femaleAdjs.txt: noun_gender: female, adjective_gender: female
[2025-04-27 17:40:09,208] INFO: 01_async_generate_sentences:511 - Successfully read the prompt from /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/prompts/prompt_femaleNouns_femaleAdjs.txt
[2025-04-27 17:40:09,212] DEBUG: _config:82 - load_ssl_context verify=True cert=None trust_env=True http2=False
[2025-04-27 17:40:09,212] DEBUG: _config:148 - load_verify_locations cafile='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/certifi/cacert.pem'
[2025-04-27 17:40:09,229] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama3-chatqa:8b': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b
[2025-04-27 17:40:09,229] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,229] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,229] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['beeldig', 'exotisch', 'voorlijk', 'sensueel', 'erotisch', 'rustiek', 'platinablond', 'donkerharig', 'stijlvol', 'tenger']
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['hitsig', 'erotisch', 'kleurig', 'sensueel', 'tenger', 'kinderloos', 'rozig', 'beeldig', 'levenslustig', 'exotisch']
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['huwelijks', 'zilvergrijs', 'rimpelig', 'erotisch', 'lesbisch', 'glamoureus', 'marokkaans', 'feministisch', 'ongehuwd', 'spichtig']
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['glamoureus', 'beeldig', 'erotisch', 'tuttig', 'beeldschoon', 'sensueel', 'hitsig', 'levenslustig', 'feministisch', 'stijlvol']
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['rustiek', 'beeldig', 'glamoureus', 'rozig', 'stijlvol', 'erotisch', 'exotisch', 'spichtig', 'kleurig', 'tenger']
[2025-04-27 17:40:09,230] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=42
[2025-04-27 17:40:09,230] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,234] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=43
[2025-04-27 17:40:09,234] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,234] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=44
[2025-04-27 17:40:09,234] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,235] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=45
[2025-04-27 17:40:09,235] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,235] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=46
[2025-04-27 17:40:09,235] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,236] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:09,236] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:09,236] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:09,236] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:09,236] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103b6aa50>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103b68f80>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103b68fe0>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103b69940>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103b6a390>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,237] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,238] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,239] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,239] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,239] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,239] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,240] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,240] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,240] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,240] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,240] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,240] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,241] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,241] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,241] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,241] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,241] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,241] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,241] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,241] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,241] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,241] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['voorlijk', 'feministisch', 'rozig', 'huwelijks', 'tuttig', 'marokkaans', 'tenger', 'exotisch', 'kinderloos', 'zilvergrijs']
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['erotisch', 'glamoureus', 'platinablond', 'ongepland', 'marokkaans', 'voorlijk', 'rustiek', 'feministisch', 'tuttig', 'tenger']
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['beeldschoon', 'rustiek', 'ongepland', 'zilvergrijs', 'teder', 'kinderloos', 'spichtig', 'rimpelig', 'voorlijk', 'ongehuwd']
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['rimpelig', 'kleurig', 'blond', 'erotisch', 'marokkaans', 'rustiek', 'glamoureus', 'huwelijks', 'donkerharig', 'stijlvol']
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['rimpelig', 'blond', 'tuttig', 'donkerharig', 'kleurig', 'hitsig', 'beeldschoon', 'exotisch', 'ongepland', 'feministisch']
[2025-04-27 17:40:09,241] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=47
[2025-04-27 17:40:09,241] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,242] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=48
[2025-04-27 17:40:09,242] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,242] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=49
[2025-04-27 17:40:09,242] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,243] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=50
[2025-04-27 17:40:09,243] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,243] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=51
[2025-04-27 17:40:09,243] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,244] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,244] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,244] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,244] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,244] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,245] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,245] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,246] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,246] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,246] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,247] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,247] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,247] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,247] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,247] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,247] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,247] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,248] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,248] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,248] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,248] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,248] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,248] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,248] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,248] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,248] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,248] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,248] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,248] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['voorlijk', 'ongepland', 'marokkaans', 'lesbisch', 'kleurig', 'rimpelig', 'donkerharig', 'teder', 'glamoureus', 'erotisch']
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['spichtig', 'tenger', 'beeldschoon', 'levenslustig', 'marokkaans', 'huwelijks', 'rustiek', 'ongehuwd', 'teder', 'feministisch']
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['ongehuwd', 'hitsig', 'levenslustig', 'tenger', 'kinderloos', 'platinablond', 'donkerharig', 'huwelijks', 'rimpelig', 'stijlvol']
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['tuttig', 'levenslustig', 'donkerharig', 'rustiek', 'tenger', 'exotisch', 'huwelijks', 'rimpelig', 'kinderloos', 'lesbisch']
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['donkerharig', 'platinablond', 'teder', 'erotisch', 'kleurig', 'ongepland', 'ongehuwd', 'glamoureus', 'tenger', 'voorlijk']
[2025-04-27 17:40:09,248] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=52
[2025-04-27 17:40:09,248] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,249] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=53
[2025-04-27 17:40:09,249] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,249] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=54
[2025-04-27 17:40:09,249] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,249] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=55
[2025-04-27 17:40:09,249] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,250] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=56
[2025-04-27 17:40:09,250] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,250] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,251] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,252] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,252] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,252] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,253] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,253] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,253] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,253] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,253] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,254] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,254] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,254] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,254] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,254] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,254] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,254] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,254] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,254] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,254] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,254] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,255] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['spichtig', 'tenger', 'bloedmooie', 'stijlvol', 'zilvergrijs', 'marokkaans', 'lesbisch', 'voorlijk', 'platinablond', 'kinderloos']
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['bloedmooie', 'feministisch', 'spichtig', 'glamoureus', 'lesbisch', 'tenger', 'kleurig', 'donkerharig', 'ongepland', 'hitsig']
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['stijlvol', 'hitsig', 'kinderloos', 'beeldig', 'beeldschoon', 'exotisch', 'levenslustig', 'huwelijks', 'glamoureus', 'sensueel']
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['sensueel', 'stijlvol', 'platinablond', 'levenslustig', 'kinderloos', 'glamoureus', 'rustiek', 'erotisch', 'rozig', 'marokkaans']
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['stijlvol', 'feministisch', 'rozig', 'blond', 'levenslustig', 'ongehuwd', 'marokkaans', 'donkerharig', 'huwelijks', 'rimpelig']
[2025-04-27 17:40:09,255] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=57
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,255] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=58
[2025-04-27 17:40:09,255] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,256] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=59
[2025-04-27 17:40:09,256] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,256] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=60
[2025-04-27 17:40:09,256] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,257] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.5 and seed=61
[2025-04-27 17:40:09,257] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,257] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,257] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,257] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,258] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,259] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,259] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,259] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,259] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,259] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,260] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,260] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,260] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,260] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,260] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,261] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,261] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,261] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,261] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,261] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,261] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,261] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.5, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,261] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,261] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:09,261] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_5_sentences_1745768409.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_5_state_1745768409.json
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp0.5.jsonl for model 'llama3-chatqa:8b' with temperature 0.5
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['ongepland', 'kleurig', 'tenger', 'beeldschoon', 'kinderloos', 'rozig', 'rimpelig', 'huwelijks', 'tuttig', 'feministisch']
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['levenslustig', 'spichtig', 'platinablond', 'kleurig', 'glamoureus', 'ongepland', 'kinderloos', 'donkerharig', 'bloedmooie', 'marokkaans']
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['ongepland', 'sensueel', 'bloedmooie', 'kinderloos', 'voorlijk', 'donkerharig', 'levenslustig', 'ongehuwd', 'erotisch', 'beeldschoon']
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['glamoureus', 'zilvergrijs', 'sensueel', 'ongepland', 'hitsig', 'blond', 'bloedmooie', 'marokkaans', 'feministisch', 'rozig']
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['lesbisch', 'kinderloos', 'voorlijk', 'feministisch', 'donkerharig', 'levenslustig', 'rozig', 'rimpelig', 'bloedmooie', 'hitsig']
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=42
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,262] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=43
[2025-04-27 17:40:09,262] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,263] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=44
[2025-04-27 17:40:09,263] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,263] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=45
[2025-04-27 17:40:09,263] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,264] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=46
[2025-04-27 17:40:09,264] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,264] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,265] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,266] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,266] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,266] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,267] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,267] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,267] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,267] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,267] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,267] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,268] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,268] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,268] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,268] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,268] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,268] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,268] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,268] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,268] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,269] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,269] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['sensueel', 'spichtig', 'rimpelig', 'lesbisch', 'blond', 'rozig', 'beeldschoon', 'donkerharig', 'hitsig', 'platinablond']
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['bloedmooie', 'ongehuwd', 'donkerharig', 'exotisch', 'sensueel', 'zilvergrijs', 'kleurig', 'tuttig', 'lesbisch', 'feministisch']
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['zilvergrijs', 'teder', 'kinderloos', 'rimpelig', 'beeldig', 'voorlijk', 'feministisch', 'glamoureus', 'ongepland', 'levenslustig']
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['voorlijk', 'marokkaans', 'glamoureus', 'ongehuwd', 'platinablond', 'sensueel', 'huwelijks', 'erotisch', 'exotisch', 'rozig']
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['huwelijks', 'kinderloos', 'glamoureus', 'stijlvol', 'zilvergrijs', 'donkerharig', 'bloedmooie', 'rustiek', 'feministisch', 'rozig']
[2025-04-27 17:40:09,269] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=47
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,269] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=48
[2025-04-27 17:40:09,269] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,270] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=49
[2025-04-27 17:40:09,270] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,270] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=50
[2025-04-27 17:40:09,270] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,271] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=51
[2025-04-27 17:40:09,271] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,271] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,271] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,271] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,271] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,271] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,272] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,273] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,273] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,273] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,273] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,273] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,274] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,274] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,274] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,274] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,274] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,275] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,275] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,275] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,275] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,275] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,275] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,275] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,275] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,275] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,275] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,275] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,275] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,275] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,275] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['stijlvol', 'feministisch', 'lesbisch', 'tenger', 'marokkaans', 'kinderloos', 'erotisch', 'levenslustig', 'bloedmooie', 'glamoureus']
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['beeldig', 'stijlvol', 'blond', 'rimpelig', 'bloedmooie', 'zilvergrijs', 'glamoureus', 'kinderloos', 'ongehuwd', 'rustiek']
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['spichtig', 'ongepland', 'beeldig', 'ongehuwd', 'blond', 'hitsig', 'zilvergrijs', 'kleurig', 'exotisch', 'huwelijks']
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['hitsig', 'beeldschoon', 'voorlijk', 'tenger', 'tuttig', 'feministisch', 'exotisch', 'sensueel', 'kleurig', 'levenslustig']
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['marokkaans', 'spichtig', 'blond', 'hitsig', 'kleurig', 'exotisch', 'bloedmooie', 'rustiek', 'tenger', 'beeldig']
[2025-04-27 17:40:09,275] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=52
[2025-04-27 17:40:09,275] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,276] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=53
[2025-04-27 17:40:09,276] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,276] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=54
[2025-04-27 17:40:09,276] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,277] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=55
[2025-04-27 17:40:09,277] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,278] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=56
[2025-04-27 17:40:09,278] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,278] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,278] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,278] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,278] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,278] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,279] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,280] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,280] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,280] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,280] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,280] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,281] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,281] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,281] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,281] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,281] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,282] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,282] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,282] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,282] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,282] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,282] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,282] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,282] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,282] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,282] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,282] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,282] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,282] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,282] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['beeldschoon', 'blond', 'tuttig', 'ongepland', 'sensueel', 'tenger', 'donkerharig', 'rozig', 'kinderloos', 'exotisch']
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['rimpelig', 'rustiek', 'beeldig', 'donkerharig', 'tuttig', 'zilvergrijs', 'glamoureus', 'platinablond', 'kleurig', 'huwelijks']
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['rustiek', 'levenslustig', 'sensueel', 'platinablond', 'glamoureus', 'erotisch', 'rozig', 'beeldig', 'spichtig', 'stijlvol']
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['zilvergrijs', 'tenger', 'erotisch', 'blond', 'ongehuwd', 'exotisch', 'lesbisch', 'sensueel', 'hitsig', 'marokkaans']
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['tenger', 'bloedmooie', 'stijlvol', 'voorlijk', 'donkerharig', 'levenslustig', 'rozig', 'kleurig', 'beeldig', 'ongehuwd']
[2025-04-27 17:40:09,282] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=57
[2025-04-27 17:40:09,282] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,283] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=58
[2025-04-27 17:40:09,283] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,283] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=59
[2025-04-27 17:40:09,283] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,284] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=60
[2025-04-27 17:40:09,284] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,284] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=0.75 and seed=61
[2025-04-27 17:40:09,284] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,285] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,286] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,286] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,286] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,287] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,287] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,287] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,287] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,287] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,288] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,288] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,288] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,289] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,289] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,289] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,289] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,289] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,289] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=0.75, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,289] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,289] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:09,289] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_75_sentences_1745768409.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp0_75_state_1745768409.json
[2025-04-27 17:40:09,289] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp0.75.jsonl for model 'llama3-chatqa:8b' with temperature 0.75
[2025-04-27 17:40:09,290] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,290] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,290] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['sensueel', 'spichtig', 'platinablond', 'tuttig', 'bloedmooie', 'donkerharig', 'huwelijks', 'tenger', 'beeldschoon', 'rimpelig']
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['donkerharig', 'tuttig', 'rustiek', 'huwelijks', 'stijlvol', 'erotisch', 'teder', 'bloedmooie', 'rimpelig', 'feministisch']
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['bloedmooie', 'feministisch', 'huwelijks', 'kinderloos', 'ongehuwd', 'exotisch', 'tenger', 'rustiek', 'platinablond', 'blond']
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['beeldig', 'tuttig', 'donkerharig', 'spichtig', 'feministisch', 'rustiek', 'erotisch', 'zilvergrijs', 'kleurig', 'kinderloos']
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['teder', 'lesbisch', 'feministisch', 'bloedmooie', 'exotisch', 'hitsig', 'rustiek', 'spichtig', 'tuttig', 'stijlvol']
[2025-04-27 17:40:09,290] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=42
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,290] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=43
[2025-04-27 17:40:09,290] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,291] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=44
[2025-04-27 17:40:09,291] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,291] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=45
[2025-04-27 17:40:09,291] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,292] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=46
[2025-04-27 17:40:09,292] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,292] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,293] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,294] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,294] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,294] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,295] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,295] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,295] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,295] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,296] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,296] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,296] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,296] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,296] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,297] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,297] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,297] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,297] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,297] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,297] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,297] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,297] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['teder', 'bloedmooie', 'blond', 'exotisch', 'beeldschoon', 'feministisch', 'lesbisch', 'spichtig', 'sensueel', 'tenger']
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['ongehuwd', 'levenslustig', 'blond', 'rustiek', 'glamoureus', 'stijlvol', 'bloedmooie', 'exotisch', 'marokkaans', 'kinderloos']
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['rozig', 'bloedmooie', 'marokkaans', 'lesbisch', 'donkerharig', 'tenger', 'hitsig', 'kinderloos', 'sensueel', 'erotisch']
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['spichtig', 'marokkaans', 'blond', 'erotisch', 'tenger', 'beeldig', 'levenslustig', 'lesbisch', 'zilvergrijs', 'tuttig']
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['rustiek', 'beeldschoon', 'voorlijk', 'levenslustig', 'feministisch', 'ongehuwd', 'kinderloos', 'donkerharig', 'erotisch', 'spichtig']
[2025-04-27 17:40:09,297] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=47
[2025-04-27 17:40:09,297] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,298] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=48
[2025-04-27 17:40:09,298] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,298] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=49
[2025-04-27 17:40:09,298] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,298] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=50
[2025-04-27 17:40:09,298] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,299] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=51
[2025-04-27 17:40:09,299] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,300] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,301] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,301] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,301] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,302] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,302] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,302] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,302] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,302] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,302] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,303] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,303] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,303] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,303] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,303] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,303] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,303] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,303] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,303] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,303] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,303] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,303] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['ongehuwd', 'marokkaans', 'zilvergrijs', 'glamoureus', 'lesbisch', 'voorlijk', 'beeldschoon', 'bloedmooie', 'rozig', 'tenger']
[2025-04-27 17:40:09,303] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['kinderloos', 'tenger', 'blond', 'lesbisch', 'rozig', 'exotisch', 'kleurig', 'erotisch', 'levenslustig', 'glamoureus']
[2025-04-27 17:40:09,303] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['marokkaans', 'erotisch', 'bloedmooie', 'spichtig', 'sensueel', 'kleurig', 'tenger', 'platinablond', 'beeldig', 'stijlvol']
[2025-04-27 17:40:09,303] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['glamoureus', 'hitsig', 'voorlijk', 'tenger', 'levenslustig', 'huwelijks', 'marokkaans', 'bloedmooie', 'donkerharig', 'beeldschoon']
[2025-04-27 17:40:09,303] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['tuttig', 'rimpelig', 'bloedmooie', 'stijlvol', 'levenslustig', 'beeldig', 'rozig', 'glamoureus', 'kleurig', 'lesbisch']
[2025-04-27 17:40:09,303] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=52
[2025-04-27 17:40:09,304] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,304] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=53
[2025-04-27 17:40:09,304] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,304] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=54
[2025-04-27 17:40:09,304] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,305] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=55
[2025-04-27 17:40:09,305] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,305] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=56
[2025-04-27 17:40:09,305] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,306] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,306] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,306] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,306] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,306] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,307] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,307] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,308] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,308] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,308] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,308] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,309] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,309] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,309] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,309] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,309] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,309] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,310] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,310] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,310] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,310] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,310] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,310] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,310] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,310] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,310] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['hitsig', 'voorlijk', 'sensueel', 'marokkaans', 'spichtig', 'ongepland', 'tenger', 'kinderloos', 'huwelijks', 'beeldschoon']
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['feministisch', 'hitsig', 'ongepland', 'donkerharig', 'beeldschoon', 'ongehuwd', 'bloedmooie', 'tenger', 'zilvergrijs', 'kinderloos']
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['hitsig', 'kinderloos', 'spichtig', 'teder', 'beeldschoon', 'exotisch', 'voorlijk', 'huwelijks', 'bloedmooie', 'ongepland']
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['kleurig', 'ongehuwd', 'marokkaans', 'kinderloos', 'levenslustig', 'ongepland', 'donkerharig', 'bloedmooie', 'lesbisch', 'stijlvol']
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['platinablond', 'zilvergrijs', 'voorlijk', 'stijlvol', 'kinderloos', 'bloedmooie', 'ongehuwd', 'rustiek', 'exotisch', 'donkerharig']
[2025-04-27 17:40:09,310] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1 and seed=57
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,310] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1 and seed=58
[2025-04-27 17:40:09,310] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,311] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1 and seed=59
[2025-04-27 17:40:09,311] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,311] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1 and seed=60
[2025-04-27 17:40:09,311] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,312] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1 and seed=61
[2025-04-27 17:40:09,312] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,312] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,312] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,313] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,314] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,314] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,314] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,314] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,315] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,315] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,315] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,315] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,315] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,316] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,316] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,316] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,316] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,316] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,316] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,316] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,316] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,316] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,317] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_sentences_1745768409.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_state_1745768409.json
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.jsonl for model 'llama3-chatqa:8b' with temperature 1
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['blond', 'kleurig', 'tenger', 'teder', 'sensueel', 'levenslustig', 'stijlvol', 'ongepland', 'platinablond', 'rustiek']
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['voorlijk', 'rozig', 'marokkaans', 'hitsig', 'beeldig', 'ongehuwd', 'rustiek', 'lesbisch', 'beeldschoon', 'kleurig']
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['exotisch', 'kleurig', 'beeldschoon', 'donkerharig', 'platinablond', 'lesbisch', 'levenslustig', 'beeldig', 'sensueel', 'spichtig']
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['ongehuwd', 'bloedmooie', 'donkerharig', 'ongepland', 'kinderloos', 'stijlvol', 'hitsig', 'kleurig', 'glamoureus', 'blond']
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['kinderloos', 'blond', 'platinablond', 'glamoureus', 'levenslustig', 'hitsig', 'kleurig', 'donkerharig', 'feministisch', 'ongepland']
[2025-04-27 17:40:09,317] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=42
[2025-04-27 17:40:09,317] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,318] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=43
[2025-04-27 17:40:09,318] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,318] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=44
[2025-04-27 17:40:09,319] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,319] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=45
[2025-04-27 17:40:09,319] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,320] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=46
[2025-04-27 17:40:09,320] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,320] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,320] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,321] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,322] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,322] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,322] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,322] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,323] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,323] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,323] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,323] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,323] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,324] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,324] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,324] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,324] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,324] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,324] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,324] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,324] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,324] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,324] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,324] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,324] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,324] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,324] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,324] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,324] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['tuttig', 'rimpelig', 'marokkaans', 'spichtig', 'teder', 'kinderloos', 'ongepland', 'zilvergrijs', 'sensueel', 'huwelijks']
[2025-04-27 17:40:09,324] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['exotisch', 'kinderloos', 'sensueel', 'beeldschoon', 'erotisch', 'hitsig', 'tuttig', 'rozig', 'stijlvol', 'rustiek']
[2025-04-27 17:40:09,324] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['levenslustig', 'donkerharig', 'rozig', 'beeldschoon', 'teder', 'glamoureus', 'platinablond', 'sensueel', 'tuttig', 'marokkaans']
[2025-04-27 17:40:09,324] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['spichtig', 'beeldig', 'blond', 'zilvergrijs', 'marokkaans', 'feministisch', 'ongepland', 'stijlvol', 'exotisch', 'voorlijk']
[2025-04-27 17:40:09,324] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['kinderloos', 'ongehuwd', 'hitsig', 'bloedmooie', 'sensueel', 'stijlvol', 'glamoureus', 'huwelijks', 'feministisch', 'erotisch']
[2025-04-27 17:40:09,325] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=47
[2025-04-27 17:40:09,325] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,325] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=48
[2025-04-27 17:40:09,325] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,326] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=49
[2025-04-27 17:40:09,326] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,326] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=50
[2025-04-27 17:40:09,326] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,327] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=51
[2025-04-27 17:40:09,327] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,327] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,327] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,327] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,328] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,329] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,329] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,329] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,329] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,330] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,330] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,330] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,330] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,331] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,331] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,331] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,331] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,332] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,332] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,332] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,332] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,332] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,332] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,332] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,332] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['teder', 'donkerharig', 'stijlvol', 'hitsig', 'beeldig', 'rozig', 'marokkaans', 'beeldschoon', 'bloedmooie', 'huwelijks']
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['beeldig', 'erotisch', 'kinderloos', 'spichtig', 'zilvergrijs', 'stijlvol', 'kleurig', 'donkerharig', 'rustiek', 'tenger']
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['beeldig', 'rimpelig', 'platinablond', 'rozig', 'tuttig', 'zilvergrijs', 'bloedmooie', 'beeldschoon', 'spichtig', 'ongehuwd']
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['ongehuwd', 'rozig', 'erotisch', 'beeldig', 'zilvergrijs', 'ongepland', 'rimpelig', 'huwelijks', 'levenslustig', 'marokkaans']
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['ongehuwd', 'stijlvol', 'beeldschoon', 'rustiek', 'rimpelig', 'voorlijk', 'tuttig', 'rozig', 'platinablond', 'erotisch']
[2025-04-27 17:40:09,332] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=52
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,332] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=53
[2025-04-27 17:40:09,332] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,333] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=54
[2025-04-27 17:40:09,333] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,333] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=55
[2025-04-27 17:40:09,333] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,334] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=56
[2025-04-27 17:40:09,334] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,334] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,335] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,336] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,336] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,336] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,336] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,337] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,337] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,337] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,337] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,337] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,338] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,338] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,338] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,338] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,338] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,338] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,338] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,338] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,338] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,338] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,338] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,338] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['beeldig', 'voorlijk', 'blond', 'stijlvol', 'teder', 'rimpelig', 'tenger', 'zilvergrijs', 'spichtig', 'beeldschoon']
[2025-04-27 17:40:09,338] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['zilvergrijs', 'beeldschoon', 'glamoureus', 'ongepland', 'bloedmooie', 'rustiek', 'donkerharig', 'tuttig', 'stijlvol', 'feministisch']
[2025-04-27 17:40:09,338] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['rozig', 'voorlijk', 'exotisch', 'hitsig', 'platinablond', 'beeldschoon', 'sensueel', 'kinderloos', 'beeldig', 'tenger']
[2025-04-27 17:40:09,339] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['rustiek', 'kinderloos', 'zilvergrijs', 'erotisch', 'teder', 'donkerharig', 'rozig', 'platinablond', 'beeldschoon', 'beeldig']
[2025-04-27 17:40:09,339] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['platinablond', 'rozig', 'hitsig', 'beeldschoon', 'tuttig', 'rimpelig', 'beeldig', 'donkerharig', 'glamoureus', 'lesbisch']
[2025-04-27 17:40:09,339] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=57
[2025-04-27 17:40:09,339] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,339] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=58
[2025-04-27 17:40:09,339] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,340] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=59
[2025-04-27 17:40:09,340] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,340] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=60
[2025-04-27 17:40:09,340] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,340] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.25 and seed=61
[2025-04-27 17:40:09,341] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,341] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,341] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,341] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,341] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,341] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,342] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,343] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,343] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,343] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,343] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,343] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,344] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,344] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,344] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,344] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,344] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,345] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,345] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,345] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,345] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,345] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,345] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,345] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.25, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,345] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,345] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:09,345] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_25_sentences_1745768409.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_25_state_1745768409.json
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.25.jsonl for model 'llama3-chatqa:8b' with temperature 1.25
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['ongehuwd', 'rimpelig', 'exotisch', 'lesbisch', 'levenslustig', 'rozig', 'voorlijk', 'tuttig', 'sensueel', 'zilvergrijs']
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['tuttig', 'stijlvol', 'sensueel', 'beeldschoon', 'erotisch', 'bloedmooie', 'feministisch', 'rimpelig', 'kleurig', 'tenger']
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['beeldschoon', 'hitsig', 'kleurig', 'tenger', 'tuttig', 'ongepland', 'bloedmooie', 'beeldig', 'glamoureus', 'donkerharig']
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['ongehuwd', 'lesbisch', 'tuttig', 'beeldschoon', 'blond', 'erotisch', 'zilvergrijs', 'levenslustig', 'glamoureus', 'exotisch']
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['marokkaans', 'rozig', 'voorlijk', 'rimpelig', 'beeldig', 'bloedmooie', 'glamoureus', 'kinderloos', 'exotisch', 'levenslustig']
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=42
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,346] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=43
[2025-04-27 17:40:09,346] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,347] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=44
[2025-04-27 17:40:09,347] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,347] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=45
[2025-04-27 17:40:09,347] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,348] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=46
[2025-04-27 17:40:09,348] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,348] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,349] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,350] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,350] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=42: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,350] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,351] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,351] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=43: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,351] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,351] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=46: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,351] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,352] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,352] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=44: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,352] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,352] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,352] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=45: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,352] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,352] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,352] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,352] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,352] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,352] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,353] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 47, Using 10 random words: ['sensueel', 'platinablond', 'erotisch', 'voorlijk', 'hitsig', 'kinderloos', 'tuttig', 'zilvergrijs', 'blond', 'beeldschoon']
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 48, Using 10 random words: ['kinderloos', 'erotisch', 'levenslustig', 'beeldschoon', 'lesbisch', 'rimpelig', 'platinablond', 'zilvergrijs', 'stijlvol', 'tenger']
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 49, Using 10 random words: ['blond', 'tuttig', 'lesbisch', 'rozig', 'zilvergrijs', 'kleurig', 'spichtig', 'sensueel', 'beeldig', 'platinablond']
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 50, Using 10 random words: ['levenslustig', 'rustiek', 'rozig', 'glamoureus', 'bloedmooie', 'kleurig', 'beeldig', 'stijlvol', 'spichtig', 'marokkaans']
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 51, Using 10 random words: ['voorlijk', 'erotisch', 'exotisch', 'rustiek', 'glamoureus', 'hitsig', 'blond', 'marokkaans', 'platinablond', 'sensueel']
[2025-04-27 17:40:09,353] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=47
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,353] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=48
[2025-04-27 17:40:09,353] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,354] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=49
[2025-04-27 17:40:09,354] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,354] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=50
[2025-04-27 17:40:09,354] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,355] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=51
[2025-04-27 17:40:09,355] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,355] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,355] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,355] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,356] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,357] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,357] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,358] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=48: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,358] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,358] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,358] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,358] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,358] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,358] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,358] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=47: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,359] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,359] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=50: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,359] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,360] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,360] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=49: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,360] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,360] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,360] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=51: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,360] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,360] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,360] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,360] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,360] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,360] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,360] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 52, Using 10 random words: ['erotisch', 'stijlvol', 'glamoureus', 'marokkaans', 'kinderloos', 'zilvergrijs', 'platinablond', 'ongehuwd', 'lesbisch', 'tenger']
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 53, Using 10 random words: ['rimpelig', 'exotisch', 'kinderloos', 'sensueel', 'platinablond', 'stijlvol', 'beeldig', 'ongepland', 'blond', 'lesbisch']
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 54, Using 10 random words: ['spichtig', 'bloedmooie', 'exotisch', 'sensueel', 'hitsig', 'beeldig', 'zilvergrijs', 'rozig', 'stijlvol', 'blond']
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 55, Using 10 random words: ['tuttig', 'erotisch', 'kinderloos', 'marokkaans', 'hitsig', 'beeldig', 'zilvergrijs', 'teder', 'levenslustig', 'donkerharig']
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 56, Using 10 random words: ['tenger', 'levenslustig', 'ongehuwd', 'tuttig', 'ongepland', 'exotisch', 'zilvergrijs', 'kinderloos', 'hitsig', 'bloedmooie']
[2025-04-27 17:40:09,360] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=52
[2025-04-27 17:40:09,360] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,361] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=53
[2025-04-27 17:40:09,361] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,361] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=54
[2025-04-27 17:40:09,361] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,362] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=55
[2025-04-27 17:40:09,362] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,362] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=56
[2025-04-27 17:40:09,362] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,363] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,364] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,364] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,365] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=52: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,365] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,365] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=53: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,365] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,365] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,366] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=56: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,366] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,366] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=54: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,366] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,366] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,367] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=55: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,367] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,367] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,367] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,367] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,367] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,367] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,367] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 57, Using 10 random words: ['kleurig', 'huwelijks', 'erotisch', 'glamoureus', 'tenger', 'kinderloos', 'exotisch', 'rimpelig', 'sensueel', 'voorlijk']
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 58, Using 10 random words: ['exotisch', 'ongehuwd', 'platinablond', 'marokkaans', 'rustiek', 'hitsig', 'sensueel', 'beeldschoon', 'beeldig', 'spichtig']
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 59, Using 10 random words: ['hitsig', 'kleurig', 'teder', 'erotisch', 'beeldig', 'sensueel', 'rimpelig', 'spichtig', 'glamoureus', 'lesbisch']
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 60, Using 10 random words: ['rimpelig', 'rozig', 'sensueel', 'huwelijks', 'kleurig', 'lesbisch', 'platinablond', 'rustiek', 'tuttig', 'stijlvol']
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 61, Using 10 random words: ['zilvergrijs', 'levenslustig', 'bloedmooie', 'stijlvol', 'ongehuwd', 'rozig', 'kleurig', 'rimpelig', 'huwelijks', 'lesbisch']
[2025-04-27 17:40:09,367] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=57
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,367] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=58
[2025-04-27 17:40:09,367] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,368] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=59
[2025-04-27 17:40:09,368] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,368] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=60
[2025-04-27 17:40:09,368] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,379] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3-chatqa:8b' with temperature=1.5 and seed=61
[2025-04-27 17:40:09,379] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,379] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,380] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,381] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,381] ERROR: 01_async_generate_sentences:368 - [Batch 0] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=57: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,381] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,382] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,382] ERROR: 01_async_generate_sentences:368 - [Batch 1] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=58: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,382] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,382] ERROR: 01_async_generate_sentences:368 - [Batch 4] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=61: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,382] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,383] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,383] ERROR: 01_async_generate_sentences:368 - [Batch 2] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=59: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Sun, 27 Apr 2025 15:40:09 GMT'), (b'Content-Length', b'70')])
[2025-04-27 17:40:09,383] INFO: _client:1786 - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 404 Not Found"
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - receive_response_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - receive_response_body.complete
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:09,383] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:09,383] ERROR: 01_async_generate_sentences:368 - [Batch 3] Model call failed for llama3-chatqa:8b, temperature=1.5, seed=60: model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:128 - Encountered errors: ['model "llama3-chatqa:8b" not found, try pulling it first (status code: 404)']
[2025-04-27 17:40:09,383] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,383] WARNING: 01_async_generate_sentences:563 - No progress after 5 batches, terminating early
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:252 - Saved intermediate results to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_5_sentences_1745768409.csv and /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/output/intermediate/llama3-chatqa_8b_female_female_temp1_5_state_1745768409.json
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:620 - Logged aggregated output to /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3-chatqa_8b/femaleNouns-femaleAdjs_temp1.5.jsonl for model 'llama3-chatqa:8b' with temperature 1.5
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama3:text': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama3_text
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama3:text' with temperature 0.5
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama3:text' with temperature 0.5
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['rustiek', 'blond', 'hitsig', 'beeldschoon', 'rimpelig', 'stijlvol', 'spichtig', 'kinderloos', 'rozig', 'kleurig']
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['rimpelig', 'levenslustig', 'bloedmooie', 'blond', 'glamoureus', 'huwelijks', 'ongepland', 'stijlvol', 'donkerharig', 'sensueel']
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['tenger', 'lesbisch', 'glamoureus', 'levenslustig', 'rimpelig', 'donkerharig', 'marokkaans', 'exotisch', 'kinderloos', 'ongepland']
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['sensueel', 'exotisch', 'rozig', 'levenslustig', 'erotisch', 'ongehuwd', 'tenger', 'kinderloos', 'kleurig', 'beeldschoon']
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['beeldschoon', 'tenger', 'feministisch', 'kinderloos', 'erotisch', 'rozig', 'ongehuwd', 'hitsig', 'blond', 'levenslustig']
[2025-04-27 17:40:09,384] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama3:text' with temperature=0.5 and seed=42
[2025-04-27 17:40:09,384] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,385] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama3:text' with temperature=0.5 and seed=43
[2025-04-27 17:40:09,385] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,385] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama3:text' with temperature=0.5 and seed=44
[2025-04-27 17:40:09,385] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,386] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama3:text' with temperature=0.5 and seed=45
[2025-04-27 17:40:09,386] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,386] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama3:text' with temperature=0.5 and seed=46
[2025-04-27 17:40:09,386] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies **10** zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **vrouw...
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,387] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:09,388] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:09,389] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:09,389] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:13,378] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - receive_response_headers.failed exception=CancelledError()
[2025-04-27 17:40:13,379] DEBUG: _trace:85 - response_closed.started
[2025-04-27 17:40:13,381] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:13,382] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:13,382] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:13,382] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:13,383] DEBUG: _trace:85 - response_closed.complete
[2025-04-27 17:40:13,384] INFO: 01_async_generate_sentences:664 - Process interrupted by user. Shutting down gracefully.
[2025-04-27 17:40:36,439] DEBUG: 01_async_generate_sentences:72 - Logger initialized with async processing. Script started.
[2025-04-27 17:40:36,440] DEBUG: selector_events:64 - Using selector: KqueueSelector
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:632 - ===== EXPERIMENT START =====
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:633 - Processing with targets: 200 total sentences, max 15 per word
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:634 - Using models: ['llama2-uncensored'] with temperatures: [1.5]
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:645 - Found 1 prompt files: ['prompt_maleNouns_femaleAdjs.txt']
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:649 - ===== PROCESSING prompt_maleNouns_femaleAdjs.txt =====
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:501 - Processing file prompt_maleNouns_femaleAdjs.txt: noun_gender: male, adjective_gender: female
[2025-04-27 17:40:36,440] INFO: 01_async_generate_sentences:511 - Successfully read the prompt from /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/prompts/prompt_maleNouns_femaleAdjs.txt
[2025-04-27 17:40:36,441] DEBUG: _config:82 - load_ssl_context verify=True cert=None trust_env=True http2=False
[2025-04-27 17:40:36,441] DEBUG: _config:148 - load_verify_locations cafile='/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/certifi/cacert.pem'
[2025-04-27 17:40:36,457] INFO: 01_async_generate_sentences:522 - Created log directory for model 'llama2-uncensored': /Users/matthijstentije/University/MSc_Data-Science/Thesis/MSc_Data_Science_Thesis/phase_02/logs/llama2-uncensored
[2025-04-27 17:40:36,457] INFO: 01_async_generate_sentences:540 - Starting aggregation for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 17:40:36,457] INFO: 01_async_generate_sentences:557 - Collected 0 sentences after 0.0 seconds
[2025-04-27 17:40:36,457] INFO: 01_async_generate_sentences:575 - Running batch of 5 prompts for model 'llama2-uncensored' with temperature 1.5
[2025-04-27 17:40:36,457] DEBUG: 01_async_generate_sentences:447 - [Batch 0] Seed: 42, Using 10 random words: ['beeldig', 'tuttig', 'feministisch', 'sensueel', 'kinderloos', 'teder', 'rimpelig', 'voorlijk', 'spichtig', 'kleurig']
[2025-04-27 17:40:36,457] DEBUG: 01_async_generate_sentences:447 - [Batch 1] Seed: 43, Using 10 random words: ['spichtig', 'bloedmooie', 'platinablond', 'feministisch', 'hitsig', 'donkerharig', 'glamoureus', 'blond', 'sensueel', 'stijlvol']
[2025-04-27 17:40:36,457] DEBUG: 01_async_generate_sentences:447 - [Batch 2] Seed: 44, Using 10 random words: ['beeldschoon', 'ongepland', 'platinablond', 'marokkaans', 'rimpelig', 'sensueel', 'tenger', 'rozig', 'lesbisch', 'spichtig']
[2025-04-27 17:40:36,457] DEBUG: 01_async_generate_sentences:447 - [Batch 3] Seed: 45, Using 10 random words: ['ongehuwd', 'teder', 'stijlvol', 'tuttig', 'marokkaans', 'hitsig', 'rimpelig', 'sensueel', 'blond', 'platinablond']
[2025-04-27 17:40:36,457] DEBUG: 01_async_generate_sentences:447 - [Batch 4] Seed: 46, Using 10 random words: ['kleurig', 'rozig', 'erotisch', 'huwelijks', 'feministisch', 'platinablond', 'rustiek', 'ongehuwd', 'ongepland', 'blond']
[2025-04-27 17:40:36,458] INFO: 01_async_generate_sentences:343 - [Batch 0] Running model 'llama2-uncensored' with temperature=1.5 and seed=42
[2025-04-27 17:40:36,458] DEBUG: 01_async_generate_sentences:344 - [Batch 0] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:40:36,463] INFO: 01_async_generate_sentences:343 - [Batch 1] Running model 'llama2-uncensored' with temperature=1.5 and seed=43
[2025-04-27 17:40:36,463] DEBUG: 01_async_generate_sentences:344 - [Batch 1] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:40:36,463] INFO: 01_async_generate_sentences:343 - [Batch 2] Running model 'llama2-uncensored' with temperature=1.5 and seed=44
[2025-04-27 17:40:36,463] DEBUG: 01_async_generate_sentences:344 - [Batch 2] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:40:36,463] INFO: 01_async_generate_sentences:343 - [Batch 3] Running model 'llama2-uncensored' with temperature=1.5 and seed=45
[2025-04-27 17:40:36,463] DEBUG: 01_async_generate_sentences:344 - [Batch 3] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:40:36,464] INFO: 01_async_generate_sentences:343 - [Batch 4] Running model 'llama2-uncensored' with temperature=1.5 and seed=46
[2025-04-27 17:40:36,464] DEBUG: 01_async_generate_sentences:344 - [Batch 4] Prompt sent to model: ## Instructies (Belangrijk: **lees ALLES vóór je begint!**)

Je taak is om precies 10 zinnen te genereren, met elk **één woord** uit de onderstaande lijst.

### 1. Genereer zinnen over een **man, mann...
[2025-04-27 17:40:36,464] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:36,464] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:36,464] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:36,464] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:36,464] DEBUG: _trace:85 - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
[2025-04-27 17:40:36,465] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1051c7290>
[2025-04-27 17:40:36,465] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105568e90>
[2025-04-27 17:40:36,465] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,465] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,465] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105568f80>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x105569940>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10556a300>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_headers.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - send_request_body.complete
[2025-04-27 17:40:36,466] DEBUG: _trace:85 - receive_response_headers.started request=<Request [b'POST']>
