{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca2253f-ac38-4d45-b181-1c9508c1f613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n",
      "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras\n",
    "%pip install \"numpy<2\"\n",
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b4e906-c80f-4479-8670-54323696d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:03,124 - INFO - Cell 2: Imports and global configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Imports & config ===\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# Environment & seed\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Logger setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Cell 2: Imports and global configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1866e65-da9e-4dfc-9dab-903681c3e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:03,131 - INFO - Cell 3: load_data() defined\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: load_data() ===\n",
    "def load_data(path=\"data/sentences_final.csv\"):\n",
    "    logger.info(f\"Loading data from {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    keep_columns = [\n",
    "        \"sentence\",\n",
    "        \"model\",\n",
    "        \"noun_gender\",\n",
    "        \"adjective_gender\",\n",
    "        \"temperature\",\n",
    "    ]\n",
    "    df = df.loc[:, [c for c in keep_columns if c in df.columns]]\n",
    "    df[\"label\"] = df.apply(\n",
    "        lambda r: \"MM\" if (r.noun_gender == \"male\" and r.adjective_gender == \"male\")\n",
    "        else \"FF\" if (r.noun_gender == \"female\" and r.adjective_gender == \"female\")\n",
    "        else \"MF\" if (r.noun_gender == \"male\" and r.adjective_gender == \"female\")\n",
    "        else \"FM\",\n",
    "        axis=1,\n",
    "    )\n",
    "    df[\"stereotype\"] = df[\"label\"].isin([\"MM\", \"FF\"]).astype(int)\n",
    "    df[\"stereotype_type\"] = df[\"stereotype\"].map({1: \"S\", 0: \"S_bar\"})\n",
    "    df[\"stratify_group\"] = df[\"stereotype\"].astype(str)\n",
    "    logger.info(f\"Loaded {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "logger.info(\"Cell 3: load_data() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6ed11c-ba74-4374-8551-b4bf771f63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:03,143 - INFO - Cell 4: metric functions defined\n",
      "2025-04-29 11:46:03,145 - INFO - Cell 5: TorchDataset defined\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: compute_metrics & compute_tpr_gap ===\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def compute_detailed_metrics(pred, metadata):\n",
    "    \"\"\"\n",
    "    Berekent gedetailleerde metrics inclusief TPR Gap per gender\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    # Basis metrics\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0 \n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        \"accuracy\": acc, \n",
    "        \"precision\": prec, \n",
    "        \"recall\": rec, \n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "    # Gender-specifieke metrics berekenen als we gender info hebben\n",
    "    if 'noun_gender' in metadata.columns:\n",
    "        # Maak subsets per gender en class\n",
    "        male_indices = metadata['noun_gender'] == 'male'\n",
    "        female_indices = metadata['noun_gender'] == 'female'\n",
    "        \n",
    "        # Stereotype klasse (positieve klasse = 1)\n",
    "        stereotype_indices = metadata['stereotype'] == 1\n",
    "        contra_indices = metadata['stereotype'] == 0\n",
    "        \n",
    "        # TPR voor mannelijke stereotypes (male + stereotype = 1)\n",
    "        male_stereotype = male_indices & stereotype_indices\n",
    "        if sum(male_stereotype) > 0:\n",
    "            tpr_male_s = accuracy_score(\n",
    "                labels[male_stereotype], \n",
    "                preds[male_stereotype]\n",
    "            )\n",
    "        else:\n",
    "            tpr_male_s = 0\n",
    "            \n",
    "        # TPR voor vrouwelijke stereotypes (female + stereotype = 1)\n",
    "        female_stereotype = female_indices & stereotype_indices\n",
    "        if sum(female_stereotype) > 0:\n",
    "            tpr_female_s = accuracy_score(\n",
    "                labels[female_stereotype], \n",
    "                preds[female_stereotype]\n",
    "            )\n",
    "        else:\n",
    "            tpr_female_s = 0\n",
    "            \n",
    "        # TPR Gap voor stereotype klasse (S)\n",
    "        tpr_gap_s = tpr_male_s - tpr_female_s\n",
    "        \n",
    "        # TPR voor mannelijke contra-stereotypes (male + contra-stereotype)\n",
    "        male_contra = male_indices & contra_indices\n",
    "        if sum(male_contra) > 0:\n",
    "            tpr_male_contra = accuracy_score(\n",
    "                labels[male_contra], \n",
    "                preds[male_contra]\n",
    "            )\n",
    "        else:\n",
    "            tpr_male_contra = 0\n",
    "            \n",
    "        # TPR voor vrouwelijke contra-stereotypes (female + contra-stereotype)\n",
    "        female_contra = female_indices & contra_indices\n",
    "        if sum(female_contra) > 0:\n",
    "            tpr_female_contra = accuracy_score(\n",
    "                labels[female_contra], \n",
    "                preds[female_contra]\n",
    "            )\n",
    "        else:\n",
    "            tpr_female_contra = 0\n",
    "            \n",
    "        # TPR Gap voor contra-stereotype klasse (contra-S)\n",
    "        tpr_gap_contra = tpr_male_contra - tpr_female_contra\n",
    "        \n",
    "        # Voeg gender-specifieke metrics toe\n",
    "        results.update({\n",
    "            \"tpr_male_s\": tpr_male_s,\n",
    "            \"tpr_female_s\": tpr_female_s,\n",
    "            \"tpr_gap_s\": tpr_gap_s,\n",
    "            \"tpr_male_contra\": tpr_male_contra,\n",
    "            \"tpr_female_contra\": tpr_female_contra,\n",
    "            \"tpr_gap_contra\": tpr_gap_contra\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "logger.info(\"Cell 4: metric functions defined\")\n",
    "# === Cell 5: TorchDataset ===\n",
    "class TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k,v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "logger.info(\"Cell 5: TorchDataset defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2db4ba8-2d0d-412d-86ec-a8ed3f9c140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:05,186 - INFO - Cell 6: run_cv_hp_search gereviseerd en gedefinieerd (minimal memory usage)\n"
     ]
    }
   ],
   "source": [
    "def run_cv_hp_search(model_name, tokenizer_name, df, n_splits=5, n_trials=5):\n",
    "    logger.info(f\"Starting CV+HPO for {model_name}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    all_results = []\n",
    "    all_details = []\n",
    "    all_grouped = []\n",
    "\n",
    "    def tokenize(df_):\n",
    "        return tokenizer(\n",
    "            df_[\"sentence\"].tolist(),\n",
    "            padding=True, truncation=True,\n",
    "            max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    for fold, (train_i, test_i) in enumerate(skf.split(df, df[\"stratify_group\"]), 1):\n",
    "        logger.info(f\"Fold {fold}/{n_splits}\")\n",
    "        train_full = df.iloc[train_i]\n",
    "        test_df = df.iloc[test_i].reset_index(drop=True)\n",
    "\n",
    "        # inner train/val split\n",
    "        try:\n",
    "            tr_df, val_df = train_test_split(\n",
    "                train_full, test_size=0.2, random_state=SEED,\n",
    "                stratify=train_full[\"stratify_group\"]\n",
    "            )\n",
    "        except ValueError:\n",
    "            tr_df, val_df = train_test_split(\n",
    "                train_full, test_size=0.2, random_state=SEED,\n",
    "                stratify=train_full[\"stereotype\"]\n",
    "            )\n",
    "        tr_df, val_df = tr_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "        # datasets\n",
    "        ds_tr = TorchDataset(tokenize(tr_df), tr_df[\"stereotype\"])\n",
    "        ds_val = TorchDataset(tokenize(val_df), val_df[\"stereotype\"])\n",
    "        ds_te = TorchDataset(tokenize(test_df), test_df[\"stereotype\"])\n",
    "\n",
    "        # trainer setup\n",
    "        model_init = lambda: AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2\n",
    "        )\n",
    "        out_b = f\"./results/{model_name.replace('/', '_')}/fold_{fold}\"\n",
    "        log_b = f\"./logs/{model_name.replace('/', '_')}/fold_{fold}\"\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=out_b,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",  # No automatic saving\n",
    "            logging_strategy=\"epoch\",\n",
    "            logging_dir=log_b,\n",
    "            report_to=\"none\",\n",
    "            fp16=True,\n",
    "            num_train_epochs=3,\n",
    "            load_best_model_at_end=False,  # No best model loading\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=32,\n",
    "            seed=SEED,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_init=model_init,\n",
    "            args=args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "        )\n",
    "\n",
    "        # Hyperparameter search\n",
    "        best = trainer.hyperparameter_search(\n",
    "            direction=\"maximize\",\n",
    "            backend=\"optuna\",\n",
    "            hp_space=lambda t: {\n",
    "                \"learning_rate\": t.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "                \"per_device_train_batch_size\": t.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "                \"weight_decay\": t.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n",
    "            },\n",
    "            n_trials=n_trials,\n",
    "            sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "        )\n",
    "        best_hyperparams = best.hyperparameters\n",
    "        logger.info(f\"Best HPO on fold {fold}: {best_hyperparams}\")\n",
    "\n",
    "        # Retrain met de beste hyperparameters\n",
    "        trainer.args.per_device_train_batch_size = best_hyperparams[\"per_device_train_batch_size\"]\n",
    "        trainer.args.learning_rate = best_hyperparams[\"learning_rate\"]\n",
    "        trainer.args.weight_decay = best_hyperparams.get(\"weight_decay\", 0.0)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Handmatig opslaan\n",
    "        trainer.save_model(f\"{out_b}/final_model\")\n",
    "\n",
    "        # Predicties en evaluatie\n",
    "        test_output = trainer.predict(test_dataset=ds_te)\n",
    "        metrics = trainer.evaluate(eval_dataset=ds_te)\n",
    "        logger.info(f\"Fold {fold} metrics: {metrics}\")\n",
    "        metrics.update({\"fold\": fold, \"model\": model_name, \"hyperparams\": best_hyperparams})\n",
    "        all_results.append(metrics)\n",
    "\n",
    "        # Gedetailleerde metrics\n",
    "        detailed = compute_detailed_metrics(test_output, test_df)\n",
    "        detailed.update({\"fold\": fold, \"model\": model_name})\n",
    "        all_details.append({**metrics, **detailed})\n",
    "\n",
    "        # Subset-analyse per model en temperatuur\n",
    "        subset_records = []\n",
    "        for model_type in test_df['model'].unique():\n",
    "            for temp in test_df['temperature'].unique():\n",
    "                subset = test_df[(test_df['model'] == model_type) & (test_df['temperature'] == temp)]\n",
    "                if len(subset) < 10:\n",
    "                    continue\n",
    "                idx = subset.index.to_numpy() - min(test_df.index)\n",
    "                preds = np.argmax(test_output.predictions[idx], axis=1)\n",
    "                labels = test_output.label_ids[idx]\n",
    "                acc = accuracy_score(labels, preds)\n",
    "                prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "                sub_det = compute_detailed_metrics(\n",
    "                    type('O', (), {'predictions': test_output.predictions[idx], 'label_ids': test_output.label_ids[idx]}),\n",
    "                    subset\n",
    "                )\n",
    "                record = {\n",
    "                    'fold': fold,\n",
    "                    'classifier_model': model_name,\n",
    "                    'llm_model': model_type,\n",
    "                    'temperature': temp,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'f1': f1,\n",
    "                    'tpr_gap_s': sub_det.get('tpr_gap_s'),\n",
    "                    'tpr_gap_contra': sub_det.get('tpr_gap_contra'),\n",
    "                    'sample_size': len(subset)\n",
    "                }\n",
    "                subset_records.append(record)\n",
    "        all_grouped.extend(subset_records)\n",
    "\n",
    "    # Samenvatten en opslaan\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    detailed_df = pd.DataFrame(all_details)\n",
    "    grouped_df = pd.DataFrame(all_grouped).groupby(['llm_model', 'temperature']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'tpr_gap_s': ['mean', 'std'],\n",
    "        'tpr_gap_contra': ['mean', 'std'],\n",
    "        'sample_size': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    results_df.to_csv(f\"results_base_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "    detailed_df.to_csv(f\"results_detailed_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "    grouped_df.to_csv(f\"results_by_model_temp_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "\n",
    "    avg_metrics = {\n",
    "        'avg_accuracy': results_df['eval_accuracy'].mean(),\n",
    "        'std_accuracy': results_df['eval_accuracy'].std(),\n",
    "        'avg_f1': results_df['eval_f1'].mean(),\n",
    "        'std_f1': results_df['eval_f1'].std(),\n",
    "        'model': model_name,\n",
    "        'hyperparams': best_hyperparams\n",
    "    }\n",
    "    logger.info(f\"Gemiddelde resultaten voor {model_name}: {avg_metrics}\")\n",
    "\n",
    "    return results_df, detailed_df, grouped_df, avg_metrics\n",
    "\n",
    "logger.info(\"Cell 6: run_cv_hp_search gereviseerd en gedefinieerd (minimal memory usage)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a4d76-3a48-4703-b28e-c024dcddb91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:09,388 - INFO - Loading data from data/sentences_final.csv\n",
      "2025-04-29 11:46:09,744 - INFO - Loaded 14295 rows\n",
      "2025-04-29 11:46:09,745 - INFO - Starting model DTAI-KULeuven/robbert-2023-dutch-large\n",
      "2025-04-29 11:46:09,746 - INFO - Starting CV+HPO for DTAI-KULeuven/robbert-2023-dutch-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95efccb3477b4629842b11e1be49802d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c697845bb2484fdfa974841094281d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/841k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eac0072d5d41eb8cd01e7dd8eb07b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/502k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb73a32fad7d4195bfcbb34d5e724c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7af83c9efd450384e490d072e584d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:46:15,724 - INFO - Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45150c39a5e45269f1bf0e2f2f5076e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/867 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cdbb87d3de4f04a08e9557df1e64a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 11:46:24,485] A new study created in memory with name: no-name-ca537256-59b7-4a22-b61b-cd962012ecde\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.294956</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.930884</td>\n",
       "      <td>0.902077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.297277</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.906383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.294645</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.878444</td>\n",
       "      <td>0.948381</td>\n",
       "      <td>0.912074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 11:57:11,404] Trial 0 finished with value: 3.6475534256623066 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 3.6475534256623066.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='572' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [572/858 04:59 < 02:30, 1.90 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.702200</td>\n",
       "      <td>0.693205</td>\n",
       "      <td>0.500437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>0.693368</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 12:02:12,943] Trial 1 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 0 with value: 3.6475534256623066.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.283060</td>\n",
       "      <td>0.896416</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.899192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>0.288049</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>0.881773</td>\n",
       "      <td>0.939633</td>\n",
       "      <td>0.909784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.206100</td>\n",
       "      <td>0.325744</td>\n",
       "      <td>0.904283</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.906530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 12:12:59,365] Trial 2 finished with value: 3.6249471646796145 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 0 with value: 3.6475534256623066.\n",
      "2025-04-29 12:12:59,366 - INFO - Best HPO on fold 1: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.294956</td>\n",
       "      <td>0.899038</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.930884</td>\n",
       "      <td>0.902077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291600</td>\n",
       "      <td>0.297277</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.906383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.294645</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.878444</td>\n",
       "      <td>0.948381</td>\n",
       "      <td>0.912074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 12:24:16,977 - INFO - Fold 1 metrics: {'eval_loss': 0.2525213062763214, 'eval_accuracy': 0.919202518363064, 'eval_precision': 0.8951187335092349, 'eval_recall': 0.9496151154653604, 'eval_f1': 0.9215619694397283, 'eval_runtime': 13.1847, 'eval_samples_per_second': 216.842, 'eval_steps_per_second': 3.413, 'epoch': 3.0}\n",
      "2025-04-29 12:24:17,105 - INFO - Fold 2/5\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 12:24:19,769] A new study created in memory with name: no-name-2618dad1-f01c-4877-a3cc-fd29c14e34c0\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 07:09 < 03:34, 2.66 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.693448</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>0.694686</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 12:31:30,982] Trial 0 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 2.665403927171633.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 07:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.348088</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.895013</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.262508</td>\n",
       "      <td>0.894668</td>\n",
       "      <td>0.870279</td>\n",
       "      <td>0.927384</td>\n",
       "      <td>0.897925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.268492</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>0.869219</td>\n",
       "      <td>0.953631</td>\n",
       "      <td>0.909470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 12:39:02,712] Trial 1 finished with value: 3.637476810652414 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 1 with value: 3.637476810652414.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 07:09 < 03:35, 2.66 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.273219</td>\n",
       "      <td>0.905594</td>\n",
       "      <td>0.868149</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.910075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.269004</td>\n",
       "      <td>0.903409</td>\n",
       "      <td>0.878489</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.906396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 12:46:14,145] Trial 2 finished with value: 3.6244269961381854 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 1 with value: 3.637476810652414.\n",
      "2025-04-29 12:46:14,147 - INFO - Best HPO on fold 2: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 07:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.348088</td>\n",
       "      <td>0.870192</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.895013</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.262508</td>\n",
       "      <td>0.894668</td>\n",
       "      <td>0.870279</td>\n",
       "      <td>0.927384</td>\n",
       "      <td>0.897925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.268492</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>0.869219</td>\n",
       "      <td>0.953631</td>\n",
       "      <td>0.909470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 12:54:13,109 - INFO - Fold 2 metrics: {'eval_loss': 0.234727680683136, 'eval_accuracy': 0.9160545645330536, 'eval_precision': 0.8862158647594278, 'eval_recall': 0.9544817927170869, 'eval_f1': 0.9190829399865138, 'eval_runtime': 12.009, 'eval_samples_per_second': 238.072, 'eval_steps_per_second': 3.747, 'epoch': 3.0}\n",
      "2025-04-29 12:54:13,235 - INFO - Fold 3/5\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 12:54:16,010] A new study created in memory with name: no-name-203386e8-fb47-420a-9d29-a4015911f4ad\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 07:09 < 03:34, 2.66 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.702600</td>\n",
       "      <td>0.693718</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.693348</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:01:27,388] Trial 0 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 2.665403927171633.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='572' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [572/858 04:59 < 02:30, 1.90 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699000</td>\n",
       "      <td>0.693877</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>0.694649</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:06:29,083] Trial 1 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 0 with value: 2.665403927171633.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.293107</td>\n",
       "      <td>0.905594</td>\n",
       "      <td>0.884647</td>\n",
       "      <td>0.932633</td>\n",
       "      <td>0.908007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.892437</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.910416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>0.910839</td>\n",
       "      <td>0.894207</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.912596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:17:15,523] Trial 2 finished with value: 3.649400641169554 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 2 with value: 3.649400641169554.\n",
      "2025-04-29 13:17:15,524 - INFO - Best HPO on fold 3: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.293107</td>\n",
       "      <td>0.905594</td>\n",
       "      <td>0.884647</td>\n",
       "      <td>0.932633</td>\n",
       "      <td>0.908007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.310600</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.892437</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.910416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.322743</td>\n",
       "      <td>0.910839</td>\n",
       "      <td>0.894207</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.912596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 13:28:34,356 - INFO - Fold 3 metrics: {'eval_loss': 0.32218459248542786, 'eval_accuracy': 0.906610703043022, 'eval_precision': 0.8757281553398059, 'eval_recall': 0.9474789915966386, 'eval_f1': 0.9101917255297679, 'eval_runtime': 13.1438, 'eval_samples_per_second': 217.517, 'eval_steps_per_second': 3.424, 'epoch': 3.0}\n",
      "2025-04-29 13:28:34,482 - INFO - Fold 4/5\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 13:28:37,016] A new study created in memory with name: no-name-551eb41a-9c6d-47d5-a45c-2f60fc738b6e\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 07:09 < 03:35, 2.66 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.694589</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.693252</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:35:48,733] Trial 0 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 2.665403927171633.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 07:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.251610</td>\n",
       "      <td>0.904283</td>\n",
       "      <td>0.883085</td>\n",
       "      <td>0.931759</td>\n",
       "      <td>0.906769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.243133</td>\n",
       "      <td>0.910839</td>\n",
       "      <td>0.876504</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.914644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.242760</td>\n",
       "      <td>0.912587</td>\n",
       "      <td>0.887428</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:43:21,252] Trial 1 finished with value: 3.660151641529221 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 1 with value: 3.660151641529221.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.244851</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.895763</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.910030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.256299</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.955381</td>\n",
       "      <td>0.916877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.289387</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.890354</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.916808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 13:54:08,685] Trial 2 finished with value: 3.666380196497797 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 2 with value: 3.666380196497797.\n",
      "2025-04-29 13:54:08,687 - INFO - Best HPO on fold 4: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.244851</td>\n",
       "      <td>0.908654</td>\n",
       "      <td>0.895763</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.910030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.256299</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.955381</td>\n",
       "      <td>0.916877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.289387</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.890354</td>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.916808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:05:25,122 - INFO - Fold 4 metrics: {'eval_loss': 0.3055974841117859, 'eval_accuracy': 0.9118572927597062, 'eval_precision': 0.8894039735099337, 'eval_recall': 0.9404761904761905, 'eval_f1': 0.9142273655547992, 'eval_runtime': 13.1521, 'eval_samples_per_second': 217.38, 'eval_steps_per_second': 3.422, 'epoch': 3.0}\n",
      "2025-04-29 14:05:25,264 - INFO - Fold 5/5\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 14:05:27,875] A new study created in memory with name: no-name-5004b9d8-59b8-4d10-98b0-daa9b2eb161b\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 07:11 < 03:36, 2.65 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.701900</td>\n",
       "      <td>0.693463</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.693334</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>0.499563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 14:12:41,373] Trial 0 finished with value: 2.665403927171633 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 2.665403927171633.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 07:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.272941</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>0.865719</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.909921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247300</td>\n",
       "      <td>0.225410</td>\n",
       "      <td>0.920017</td>\n",
       "      <td>0.915225</td>\n",
       "      <td>0.925634</td>\n",
       "      <td>0.920400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.233855</td>\n",
       "      <td>0.919143</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.933508</td>\n",
       "      <td>0.920224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 14:20:14,605] Trial 1 finished with value: 3.6801888278583643 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 1 with value: 3.6801888278583643.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>0.284589</td>\n",
       "      <td>0.909965</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.957130</td>\n",
       "      <td>0.913952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.233662</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.905439</td>\n",
       "      <td>0.946632</td>\n",
       "      <td>0.925577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.309490</td>\n",
       "      <td>0.919580</td>\n",
       "      <td>0.906012</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.920826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 14:31:02,974] Trial 2 finished with value: 3.682551419108226 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 2 with value: 3.682551419108226.\n",
      "2025-04-29 14:31:02,976 - INFO - Best HPO on fold 5: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DTAI-KULeuven/robbert-2023-dutch-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 10:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>0.284589</td>\n",
       "      <td>0.909965</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.957130</td>\n",
       "      <td>0.913952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.233662</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.905439</td>\n",
       "      <td>0.946632</td>\n",
       "      <td>0.925577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.309490</td>\n",
       "      <td>0.919580</td>\n",
       "      <td>0.906012</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.920826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 14:42:16,283 - INFO - Fold 5 metrics: {'eval_loss': 0.37825778126716614, 'eval_accuracy': 0.9024134312696747, 'eval_precision': 0.8767213114754099, 'eval_recall': 0.9362745098039216, 'eval_f1': 0.9055198103623434, 'eval_runtime': 11.3128, 'eval_samples_per_second': 252.722, 'eval_steps_per_second': 3.978, 'epoch': 3.0}\n",
      "2025-04-29 14:42:16,434 - INFO - Gemiddelde resultaten voor DTAI-KULeuven/robbert-2023-dutch-large: {'avg_accuracy': 0.9112277019937041, 'std_accuracy': 0.0068246012766040465, 'avg_f1': 0.9141167621746306, 'std_f1': 0.006508067292090506, 'model': 'DTAI-KULeuven/robbert-2023-dutch-large', 'hyperparams': {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}}\n",
      "2025-04-29 14:42:16,527 - INFO - All models finished\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    models = [\n",
    "        #(\"GroNLP/bert-base-dutch-cased\", \"GroNLP/bert-base-dutch-cased\"),\n",
    "        (\"distilbert/distilbert-base-multilingual-cased\", \"distilbert/distilbert-base-multilingual-cased\"),\n",
    "        #(\"DTAI-KULeuven/robbert-2023-dutch-large\", \"DTAI-KULeuven/robbert-2023-dutch-large\"),\n",
    "    ]\n",
    "    # Resultaten opslaan per model\n",
    "    all_summary_metrics = []\n",
    "    all_detailed_results_dfs = []\n",
    "    all_grouped_results = []\n",
    "    \n",
    "    # Voor elk model\n",
    "    for model_name, tokenizer_name in models:\n",
    "        logger.info(f\"Starting model {model_name}\")\n",
    "        _, detailed_results_df, grouped_results, summary_metrics = run_cv_hp_search(\n",
    "            model_name, tokenizer_name, df, n_splits=5, n_trials=3\n",
    "        )\n",
    "        all_summary_metrics.append(summary_metrics)\n",
    "        all_detailed_results_dfs.append(detailed_results_df)\n",
    "        all_grouped_results.append(grouped_results)\n",
    "    \n",
    "    # Alle resultaten combineren en vergelijken\n",
    "    summary_df = pd.DataFrame(all_summary_metrics)\n",
    "    summary_df.to_csv(\"all_models_comparison.csv\", index=False)\n",
    "    \n",
    "    # Combineer alle gedetailleerde resultaten\n",
    "    combined_detailed = pd.concat(all_detailed_results_dfs)\n",
    "    combined_detailed.to_csv(\"all_detailed_results.csv\", index=False)\n",
    "\n",
    "    logger.info(\"All models finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
