{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27b5f6-af2d-4ed4-86c8-b487cdd7ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /usr/local/lib/python3.10/dist-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.41.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras\n",
    "%pip install \"numpy<2\"\n",
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4e906-c80f-4479-8670-54323696d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:16:44.260948: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-29 10:16:44.264331: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-29 10:16:44.275156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745921804.295778   24006 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745921804.302258   24006 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745921804.318059   24006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745921804.318080   24006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745921804.318083   24006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745921804.318085   24006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-29 10:16:44.323363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 10:16:46,253 - INFO - Cell 2: Imports and global configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Imports & config ===\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# Environment & seed\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Logger setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Cell 2: Imports and global configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1866e65-da9e-4dfc-9dab-903681c3e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:16:46,261 - INFO - Cell 3: load_data() defined\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: load_data() ===\n",
    "def load_data(path=\"data/sentences_final.csv\"):\n",
    "    logger.info(f\"Loading data from {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    keep_columns = [\n",
    "        \"sentence\",\n",
    "        \"model\",\n",
    "        \"noun_gender\",\n",
    "        \"adjective_gender\",\n",
    "        \"temperature\",\n",
    "    ]\n",
    "    df = df.loc[:, [c for c in keep_columns if c in df.columns]]\n",
    "    df[\"label\"] = df.apply(\n",
    "        lambda r: \"MM\" if (r.noun_gender == \"male\" and r.adjective_gender == \"male\")\n",
    "        else \"FF\" if (r.noun_gender == \"female\" and r.adjective_gender == \"female\")\n",
    "        else \"MF\" if (r.noun_gender == \"male\" and r.adjective_gender == \"female\")\n",
    "        else \"FM\",\n",
    "        axis=1,\n",
    "    )\n",
    "    df[\"stereotype\"] = df[\"label\"].isin([\"MM\", \"FF\"]).astype(int)\n",
    "    df[\"stereotype_type\"] = df[\"stereotype\"].map({1: \"S\", 0: \"S_bar\"})\n",
    "    df[\"stratify_group\"] = df[\"stereotype\"].astype(str)\n",
    "    logger.info(f\"Loaded {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "logger.info(\"Cell 3: load_data() defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6ed11c-ba74-4374-8551-b4bf771f63e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:16:46,274 - INFO - Cell 4: metric functions defined\n",
      "2025-04-29 10:16:46,276 - INFO - Cell 5: TorchDataset defined\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: compute_metrics & compute_tpr_gap ===\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "def compute_detailed_metrics(pred, metadata):\n",
    "    \"\"\"\n",
    "    Berekent gedetailleerde metrics inclusief TPR Gap per gender\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    # Basis metrics\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\", pos_label=1, zero_division=0 \n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        \"accuracy\": acc, \n",
    "        \"precision\": prec, \n",
    "        \"recall\": rec, \n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "    # Gender-specifieke metrics berekenen als we gender info hebben\n",
    "    if 'noun_gender' in metadata.columns:\n",
    "        # Maak subsets per gender en class\n",
    "        male_indices = metadata['noun_gender'] == 'male'\n",
    "        female_indices = metadata['noun_gender'] == 'female'\n",
    "        \n",
    "        # Stereotype klasse (positieve klasse = 1)\n",
    "        stereotype_indices = metadata['stereotype'] == 1\n",
    "        contra_indices = metadata['stereotype'] == 0\n",
    "        \n",
    "        # TPR voor mannelijke stereotypes (male + stereotype = 1)\n",
    "        male_stereotype = male_indices & stereotype_indices\n",
    "        if sum(male_stereotype) > 0:\n",
    "            tpr_male_s = accuracy_score(\n",
    "                labels[male_stereotype], \n",
    "                preds[male_stereotype]\n",
    "            )\n",
    "        else:\n",
    "            tpr_male_s = 0\n",
    "            \n",
    "        # TPR voor vrouwelijke stereotypes (female + stereotype = 1)\n",
    "        female_stereotype = female_indices & stereotype_indices\n",
    "        if sum(female_stereotype) > 0:\n",
    "            tpr_female_s = accuracy_score(\n",
    "                labels[female_stereotype], \n",
    "                preds[female_stereotype]\n",
    "            )\n",
    "        else:\n",
    "            tpr_female_s = 0\n",
    "            \n",
    "        # TPR Gap voor stereotype klasse (S)\n",
    "        tpr_gap_s = tpr_male_s - tpr_female_s\n",
    "        \n",
    "        # TPR voor mannelijke contra-stereotypes (male + contra-stereotype)\n",
    "        male_contra = male_indices & contra_indices\n",
    "        if sum(male_contra) > 0:\n",
    "            tpr_male_contra = accuracy_score(\n",
    "                labels[male_contra], \n",
    "                preds[male_contra]\n",
    "            )\n",
    "        else:\n",
    "            tpr_male_contra = 0\n",
    "            \n",
    "        # TPR voor vrouwelijke contra-stereotypes (female + contra-stereotype)\n",
    "        female_contra = female_indices & contra_indices\n",
    "        if sum(female_contra) > 0:\n",
    "            tpr_female_contra = accuracy_score(\n",
    "                labels[female_contra], \n",
    "                preds[female_contra]\n",
    "            )\n",
    "        else:\n",
    "            tpr_female_contra = 0\n",
    "            \n",
    "        # TPR Gap voor contra-stereotype klasse (contra-S)\n",
    "        tpr_gap_contra = tpr_male_contra - tpr_female_contra\n",
    "        \n",
    "        # Voeg gender-specifieke metrics toe\n",
    "        results.update({\n",
    "            \"tpr_male_s\": tpr_male_s,\n",
    "            \"tpr_female_s\": tpr_female_s,\n",
    "            \"tpr_gap_s\": tpr_gap_s,\n",
    "            \"tpr_male_contra\": tpr_male_contra,\n",
    "            \"tpr_female_contra\": tpr_female_contra,\n",
    "            \"tpr_gap_contra\": tpr_gap_contra\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "logger.info(\"Cell 4: metric functions defined\")\n",
    "# === Cell 5: TorchDataset ===\n",
    "class TorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k,v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "logger.info(\"Cell 5: TorchDataset defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db4ba8-2d0d-412d-86ec-a8ed3f9c140c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:17:44,966 - INFO - Cell 6: run_cv_hp_search gereviseerd en gedefinieerd (minimal memory usage)\n"
     ]
    }
   ],
   "source": [
    "def run_cv_hp_search(model_name, tokenizer_name, df, n_splits=5, n_trials=5):\n",
    "    logger.info(f\"Starting CV+HPO for {model_name}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    all_results = []\n",
    "    all_details = []\n",
    "    all_grouped = []\n",
    "\n",
    "    def tokenize(df_):\n",
    "        return tokenizer(\n",
    "            df_[\"sentence\"].tolist(),\n",
    "            padding=True, truncation=True,\n",
    "            max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    for fold, (train_i, test_i) in enumerate(skf.split(df, df[\"stratify_group\"]), 1):\n",
    "        logger.info(f\"Fold {fold}/{n_splits}\")\n",
    "        train_full = df.iloc[train_i]\n",
    "        test_df = df.iloc[test_i].reset_index(drop=True)\n",
    "\n",
    "        # inner train/val split\n",
    "        try:\n",
    "            tr_df, val_df = train_test_split(\n",
    "                train_full, test_size=0.2, random_state=SEED,\n",
    "                stratify=train_full[\"stratify_group\"]\n",
    "            )\n",
    "        except ValueError:\n",
    "            tr_df, val_df = train_test_split(\n",
    "                train_full, test_size=0.2, random_state=SEED,\n",
    "                stratify=train_full[\"stereotype\"]\n",
    "            )\n",
    "        tr_df, val_df = tr_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "\n",
    "        # datasets\n",
    "        ds_tr = TorchDataset(tokenize(tr_df), tr_df[\"stereotype\"])\n",
    "        ds_val = TorchDataset(tokenize(val_df), val_df[\"stereotype\"])\n",
    "        ds_te = TorchDataset(tokenize(test_df), test_df[\"stereotype\"])\n",
    "\n",
    "        # trainer setup\n",
    "        model_init = lambda: AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2\n",
    "        )\n",
    "        out_b = f\"./results/{model_name.replace('/', '_')}/fold_{fold}\"\n",
    "        log_b = f\"./logs/{model_name.replace('/', '_')}/fold_{fold}\"\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=out_b,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",  \n",
    "            logging_strategy=\"epoch\",\n",
    "            logging_dir=log_b,\n",
    "            report_to=\"none\",\n",
    "            fp16=True,\n",
    "            num_train_epochs=3,\n",
    "            load_best_model_at_end=False, \n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=32,\n",
    "            seed=SEED,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_init=model_init,\n",
    "            args=args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "        )\n",
    "\n",
    "        # Hyperparameter search\n",
    "        best = trainer.hyperparameter_search(\n",
    "            direction=\"maximize\",\n",
    "            backend=\"optuna\",\n",
    "            hp_space=lambda t: {\n",
    "                \"learning_rate\": t.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "                \"per_device_train_batch_size\": t.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "                \"weight_decay\": t.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True),\n",
    "            },\n",
    "            n_trials=n_trials,\n",
    "            sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "        )\n",
    "        best_hyperparams = best.hyperparameters\n",
    "        logger.info(f\"Best HPO on fold {fold}: {best_hyperparams}\")\n",
    "\n",
    "        # Retrain met de beste hyperparameters\n",
    "        trainer.args.per_device_train_batch_size = best_hyperparams[\"per_device_train_batch_size\"]\n",
    "        trainer.args.learning_rate = best_hyperparams[\"learning_rate\"]\n",
    "        trainer.args.weight_decay = best_hyperparams.get(\"weight_decay\", 0.0)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Handmatig opslaan\n",
    "        trainer.save_model(f\"{out_b}/final_model\")\n",
    "\n",
    "        # Predicties en evaluatie\n",
    "        test_output = trainer.predict(test_dataset=ds_te)\n",
    "        metrics = trainer.evaluate(eval_dataset=ds_te)\n",
    "        logger.info(f\"Fold {fold} metrics: {metrics}\")\n",
    "        metrics.update({\"fold\": fold, \"model\": model_name, \"hyperparams\": best_hyperparams})\n",
    "        all_results.append(metrics)\n",
    "\n",
    "        # Gedetailleerde metrics\n",
    "        detailed = compute_detailed_metrics(test_output, test_df)\n",
    "        detailed.update({\"fold\": fold, \"model\": model_name})\n",
    "        all_details.append({**metrics, **detailed})\n",
    "\n",
    "        # Subset-analyse per model en temperatuur\n",
    "        subset_records = []\n",
    "        for model_type in test_df['model'].unique():\n",
    "            for temp in test_df['temperature'].unique():\n",
    "                subset = test_df[(test_df['model'] == model_type) & (test_df['temperature'] == temp)]\n",
    "                if len(subset) < 10:\n",
    "                    continue\n",
    "                idx = subset.index.to_numpy() - min(test_df.index)\n",
    "                preds = np.argmax(test_output.predictions[idx], axis=1)\n",
    "                labels = test_output.label_ids[idx]\n",
    "                acc = accuracy_score(labels, preds)\n",
    "                prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "                sub_det = compute_detailed_metrics(\n",
    "                    type('O', (), {'predictions': test_output.predictions[idx], 'label_ids': test_output.label_ids[idx]}),\n",
    "                    subset\n",
    "                )\n",
    "                record = {\n",
    "                    'fold': fold,\n",
    "                    'classifier_model': model_name,\n",
    "                    'llm_model': model_type,\n",
    "                    'temperature': temp,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'f1': f1,\n",
    "                    'tpr_gap_s': sub_det.get('tpr_gap_s'),\n",
    "                    'tpr_gap_contra': sub_det.get('tpr_gap_contra'),\n",
    "                    'sample_size': len(subset)\n",
    "                }\n",
    "                subset_records.append(record)\n",
    "        all_grouped.extend(subset_records)\n",
    "\n",
    "    # Samenvatten en opslaan\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    detailed_df = pd.DataFrame(all_details)\n",
    "    grouped_df = pd.DataFrame(all_grouped).groupby(['llm_model', 'temperature']).agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'f1': ['mean', 'std'],\n",
    "        'tpr_gap_s': ['mean', 'std'],\n",
    "        'tpr_gap_contra': ['mean', 'std'],\n",
    "        'sample_size': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    results_df.to_csv(f\"results_base_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "    detailed_df.to_csv(f\"results_detailed_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "    grouped_df.to_csv(f\"results_by_model_temp_{model_name.replace('/', '_')}.csv\", index=False)\n",
    "\n",
    "    avg_metrics = {\n",
    "        'avg_accuracy': results_df['eval_accuracy'].mean(),\n",
    "        'std_accuracy': results_df['eval_accuracy'].std(),\n",
    "        'avg_f1': results_df['eval_f1'].mean(),\n",
    "        'std_f1': results_df['eval_f1'].std(),\n",
    "        'model': model_name,\n",
    "        'hyperparams': best_hyperparams\n",
    "    }\n",
    "    logger.info(f\"Gemiddelde resultaten voor {model_name}: {avg_metrics}\")\n",
    "\n",
    "    return results_df, detailed_df, grouped_df, avg_metrics\n",
    "\n",
    "logger.info(\"Cell 6: run_cv_hp_search\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a4d76-3a48-4703-b28e-c024dcddb91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:17:49,011 - INFO - Loading data from data/sentences_final.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14295\n",
      "14295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:17:49,339 - INFO - Loaded 14295 rows\n",
      "2025-04-29 10:17:49,339 - INFO - Starting model GroNLP/bert-base-dutch-cased\n",
      "2025-04-29 10:17:49,340 - INFO - Starting CV+HPO for GroNLP/bert-base-dutch-cased\n",
      "2025-04-29 10:17:49,505 - INFO - Fold 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14295\n",
      "14295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 10:17:52,307] A new study created in memory with name: no-name-fb0f31d9-4165-4605-8211-c2879af1f433\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1716 03:35 < 00:15, 7.44 it/s, Epoch 2.80/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.245543</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.874290</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.289819</td>\n",
       "      <td>0.904283</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.907243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 10:28:10,883] Trial 2 finished with value: 3.6302842630428467 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 0 with value: 3.652381713770728.\n",
      "2025-04-29 10:28:10,885 - INFO - Best HPO on fold 1: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1182' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1182/1716 02:39 < 01:12, 7.38 it/s, Epoch 2.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.245543</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.874290</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.289819</td>\n",
       "      <td>0.904283</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.937008</td>\n",
       "      <td>0.907243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[I 2025-04-29 10:41:15,372] Trial 2 finished with value: 3.6274512628435613 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 1 with value: 3.6293247919326226.\n",
      "2025-04-29 10:41:15,373 - INFO - Best HPO on fold 2: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 02:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.304341</td>\n",
       "      <td>0.885927</td>\n",
       "      <td>0.851675</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>0.891114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.258221</td>\n",
       "      <td>0.904720</td>\n",
       "      <td>0.881286</td>\n",
       "      <td>0.935258</td>\n",
       "      <td>0.907470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.266516</td>\n",
       "      <td>0.905157</td>\n",
       "      <td>0.883914</td>\n",
       "      <td>0.932633</td>\n",
       "      <td>0.907620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 10:43:52,683 - INFO - Fold 2 metrics: {'eval_loss': 0.2523389458656311, 'eval_accuracy': 0.9073102483385799, 'eval_precision': 0.8884435537742151, 'eval_recall': 0.9313725490196079, 'eval_f1': 0.9094017094017094, 'eval_runtime': 3.6904, 'eval_samples_per_second': 774.708, 'eval_steps_per_second': 12.194, 'epoch': 3.0}\n",
      "2025-04-29 10:43:52,808 - INFO - Fold 3/5\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 10:43:55,103] A new study created in memory with name: no-name-64c57eee-cf96-421b-94bd-63810b225228\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.284705</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.893926</td>\n",
       "      <td>0.914261</td>\n",
       "      <td>0.903979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.313696</td>\n",
       "      <td>0.907343</td>\n",
       "      <td>0.900947</td>\n",
       "      <td>0.915136</td>\n",
       "      <td>0.907986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.387812</td>\n",
       "      <td>0.906906</td>\n",
       "      <td>0.900862</td>\n",
       "      <td>0.914261</td>\n",
       "      <td>0.907512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[I 2025-04-29 10:47:51,306] Trial 0 finished with value: 3.6295403217280264 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 3.6295403217280264.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 02:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.488900</td>\n",
       "      <td>0.292796</td>\n",
       "      <td>0.891171</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.913386</td>\n",
       "      <td>0.893453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.269039</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.889924</td>\n",
       "      <td>0.919510</td>\n",
       "      <td>0.904475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.273472</td>\n",
       "      <td>0.904283</td>\n",
       "      <td>0.889545</td>\n",
       "      <td>0.923010</td>\n",
       "      <td>0.905968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 10:50:20,911] Trial 1 finished with value: 3.6228057553139648 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 0 with value: 3.6295403217280264.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.291581</td>\n",
       "      <td>0.895542</td>\n",
       "      <td>0.887650</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.896492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.293848</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.891915</td>\n",
       "      <td>0.916885</td>\n",
       "      <td>0.904228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.890585</td>\n",
       "      <td>0.918635</td>\n",
       "      <td>0.904393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 10:54:16,581] Trial 2 finished with value: 3.6165852051638634 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 0 with value: 3.6295403217280264.\n",
      "2025-04-29 10:54:16,582 - INFO - Best HPO on fold 3: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 884/1716 01:59 < 01:52, 7.41 it/s, Epoch 1.54/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.284705</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.893926</td>\n",
       "      <td>0.914261</td>\n",
       "      <td>0.903979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[I 2025-04-29 11:03:30,611] Trial 1 finished with value: 3.6305681075673295 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 0 with value: 3.651764062235709.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.269857</td>\n",
       "      <td>0.901224</td>\n",
       "      <td>0.880498</td>\n",
       "      <td>0.928259</td>\n",
       "      <td>0.903748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244600</td>\n",
       "      <td>0.264416</td>\n",
       "      <td>0.909528</td>\n",
       "      <td>0.881729</td>\n",
       "      <td>0.945757</td>\n",
       "      <td>0.912621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.189400</td>\n",
       "      <td>0.280305</td>\n",
       "      <td>0.908217</td>\n",
       "      <td>0.883951</td>\n",
       "      <td>0.939633</td>\n",
       "      <td>0.910941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 11:07:27,704] Trial 2 finished with value: 3.6427414222594647 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 0 with value: 3.651764062235709.\n",
      "2025-04-29 11:07:27,705 - INFO - Best HPO on fold 4: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1144' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1144/1716 02:35 < 01:17, 7.34 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.240284</td>\n",
       "      <td>0.912587</td>\n",
       "      <td>0.884898</td>\n",
       "      <td>0.948381</td>\n",
       "      <td>0.915541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.260476</td>\n",
       "      <td>0.910402</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.943132</td>\n",
       "      <td>0.913172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:10:13,244 - INFO - Fold 4 metrics: {'eval_loss': 0.26728877425193787, 'eval_accuracy': 0.9122070654074851, 'eval_precision': 0.8859016393442622, 'eval_recall': 0.946078431372549, 'eval_f1': 0.9150016931933627, 'eval_runtime': 4.0132, 'eval_samples_per_second': 712.408, 'eval_steps_per_second': 11.213, 'epoch': 2.0}\n",
      "2025-04-29 11:10:13,370 - INFO - Fold 5/5\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-04-29 11:10:15,598] A new study created in memory with name: no-name-79bfca5e-a66e-41bd-8a4a-633be6334f75\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.260767</td>\n",
       "      <td>0.907343</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.947507</td>\n",
       "      <td>0.910849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.246083</td>\n",
       "      <td>0.916958</td>\n",
       "      <td>0.899413</td>\n",
       "      <td>0.938758</td>\n",
       "      <td>0.918664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.347054</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.896899</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.916096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 11:14:09,559] Trial 0 finished with value: 3.663463113144657 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}. Best is trial 0 with value: 3.663463113144657.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='858' max='858' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [858/858 02:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495200</td>\n",
       "      <td>0.299901</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>0.853717</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>0.892231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.253327</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.884263</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.906143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.261273</td>\n",
       "      <td>0.903409</td>\n",
       "      <td>0.884167</td>\n",
       "      <td>0.928259</td>\n",
       "      <td>0.905676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 11:16:38,676] Trial 1 finished with value: 3.621511208346076 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0006796578090758161}. Best is trial 0 with value: 3.663463113144657.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.281028</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.864188</td>\n",
       "      <td>0.935258</td>\n",
       "      <td>0.898319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.253637</td>\n",
       "      <td>0.909528</td>\n",
       "      <td>0.886139</td>\n",
       "      <td>0.939633</td>\n",
       "      <td>0.912102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.289729</td>\n",
       "      <td>0.907343</td>\n",
       "      <td>0.885029</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.909864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[I 2025-04-29 11:20:32,439] Trial 2 finished with value: 3.638368535843045 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 8, 'weight_decay': 5.337032762603957e-06}. Best is trial 0 with value: 3.663463113144657.\n",
      "2025-04-29 11:20:32,440 - INFO - Best HPO on fold 5: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1716' max='1716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1716/1716 03:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.260767</td>\n",
       "      <td>0.907343</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.947507</td>\n",
       "      <td>0.910849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.246083</td>\n",
       "      <td>0.916958</td>\n",
       "      <td>0.899413</td>\n",
       "      <td>0.938758</td>\n",
       "      <td>0.918664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.347054</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.896899</td>\n",
       "      <td>0.936133</td>\n",
       "      <td>0.916096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:24:35,717 - INFO - Fold 5 metrics: {'eval_loss': 0.4026331603527069, 'eval_accuracy': 0.9076600209863589, 'eval_precision': 0.885430463576159, 'eval_recall': 0.9362745098039216, 'eval_f1': 0.910142954390742, 'eval_runtime': 3.4412, 'eval_samples_per_second': 830.822, 'eval_steps_per_second': 13.077, 'epoch': 3.0}\n",
      "2025-04-29 11:24:35,866 - INFO - Gemiddelde resultaten voor GroNLP/bert-base-dutch-cased: {'avg_accuracy': 0.9084295208114727, 'std_accuracy': 0.008551208043239668, 'avg_f1': 0.9107731026739634, 'std_f1': 0.00838172754220645, 'model': 'GroNLP/bert-base-dutch-cased', 'hyperparams': {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 8, 'weight_decay': 4.2079886696066345e-06}}\n",
      "2025-04-29 11:24:35,958 - INFO - All models finished\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    models = [\n",
    "        (\"GroNLP/bert-base-dutch-cased\", \"GroNLP/bert-base-dutch-cased\"),\n",
    "        #(\"bert-base-multilingual-cased\", \"bert-base-multilingual-cased\"),\n",
    "        #(\"DTAI-KULeuven/robbert-2023-dutch-large\", \"DTAI-KULeuven/robbert-2023-dutch-large\"),\n",
    "    ]\n",
    "    # Resultaten opslaan per model\n",
    "    all_summary_metrics = []\n",
    "    all_detailed_results_dfs = []\n",
    "    all_grouped_results = []\n",
    "    \n",
    "    # Voor elk model\n",
    "    for model_name, tokenizer_name in models:\n",
    "        logger.info(f\"Starting model {model_name}\")\n",
    "        _, detailed_results_df, grouped_results, summary_metrics = run_cv_hp_search(\n",
    "            model_name, tokenizer_name, df, n_splits=5, n_trials=3\n",
    "        )\n",
    "        all_summary_metrics.append(summary_metrics)\n",
    "        all_detailed_results_dfs.append(detailed_results_df)\n",
    "        all_grouped_results.append(grouped_results)\n",
    "    \n",
    "    # Alle resultaten combineren en vergelijken\n",
    "    summary_df = pd.DataFrame(all_summary_metrics)\n",
    "    summary_df.to_csv(\"all_models_comparison.csv\", index=False)\n",
    "    \n",
    "    # Combineer alle gedetailleerde resultaten\n",
    "    combined_detailed = pd.concat(all_detailed_results_dfs)\n",
    "    combined_detailed.to_csv(\"all_detailed_results.csv\", index=False)\n",
    "\n",
    "    logger.info(\"All models finished\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
